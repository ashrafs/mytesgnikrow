<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<!-- saved from url=(0057)http://en.wikipedia.org/wiki/Principal_component_analysis -->
<HTML lang=en dir=ltr xmlns="http://www.w3.org/1999/xhtml"><HEAD><TITLE>Principal component analysis - Wikipedia, the free encyclopedia</TITLE>
<META http-equiv=Content-Type content="text/html; charset=UTF-8">
<META http-equiv=Content-Style-Type content=text/css>
<META content="MSHTML 6.00.6000.20710" name=GENERATOR><LINK 
title="Edit this page" 
href="/w/index.php?title=Principal_component_analysis&amp;action=edit" 
type=application/x-wiki rel=alternate><LINK title="Edit this page" 
href="/w/index.php?title=Principal_component_analysis&amp;action=edit" 
rel=edit><LINK href="http://en.wikipedia.org/apple-touch-icon.png" 
rel=apple-touch-icon><LINK href="/favicon.ico" rel="shortcut icon"><LINK 
title="Wikipedia (en)" href="/w/opensearch_desc.php" 
type=application/opensearchdescription+xml rel=search><LINK 
href="http://creativecommons.org/licenses/by-sa/3.0/" rel=copyright><LINK 
title="Wikipedia Atom feed" 
href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" 
type=application/atom+xml rel=alternate><LINK media=screen 
href="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/main-ltr.css" 
type=text/css rel=stylesheet><LINK media=screen 
href="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/shared.css" 
type=text/css rel=stylesheet><LINK media=print 
href="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/commonPrint.css" 
type=text/css rel=stylesheet><LINK media=all 
href="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/combined.min.css" 
type=text/css rel=stylesheet><LINK media=all 
href="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/jquery-ui-1.7.2.css" 
type=text/css rel=stylesheet><LINK media=all 
href="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/index.css" 
type=text/css rel=stylesheet><LINK media=print 
href="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/index(1).css" 
type=text/css rel=stylesheet><LINK media=handheld 
href="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/index(2).css" 
type=text/css rel=stylesheet><LINK media=all 
href="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/index(3).css" 
type=text/css rel=stylesheet><LINK media=all 
href="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/index(4).css" 
type=text/css rel=stylesheet>
<SCRIPT type=text/javascript>
var skin="vector",
stylepath="http://bits.wikimedia.org/skins-1.5",
wgUrlProtocols="http\\:\\/\\/|https\\:\\/\\/|ftp\\:\\/\\/|irc\\:\\/\\/|gopher\\:\\/\\/|telnet\\:\\/\\/|nntp\\:\\/\\/|worldwind\\:\\/\\/|mailto\\:|news\\:|svn\\:\\/\\/",
wgArticlePath="/wiki/$1",
wgScriptPath="/w",
wgScriptExtension=".php",
wgScript="/w/index.php",
wgVariantArticlePath=false,
wgActionPaths={},
wgServer="http://en.wikipedia.org",
wgCanonicalNamespace="",
wgCanonicalSpecialPageName=false,
wgNamespaceNumber=0,
wgPageName="Principal_component_analysis",
wgTitle="Principal component analysis",
wgAction="view",
wgArticleId=76340,
wgIsArticle=true,
wgUserName=null,
wgUserGroups=null,
wgUserLanguage="en",
wgContentLanguage="en",
wgBreakFrames=false,
wgCurRevisionId=371858494,
wgVersion="1.16wmf4",
wgEnableAPI=true,
wgEnableWriteAPI=true,
wgSeparatorTransformTable=["", ""],
wgDigitTransformTable=["", ""],
wgMainPageTitle="Main Page",
wgFormattedNamespaces={"-2": "Media", "-1": "Special", "0": "", "1": "Talk", "2": "User", "3": "User talk", "4": "Wikipedia", "5": "Wikipedia talk", "6": "File", "7": "File talk", "8": "MediaWiki", "9": "MediaWiki talk", "10": "Template", "11": "Template talk", "12": "Help", "13": "Help talk", "14": "Category", "15": "Category talk", "100": "Portal", "101": "Portal talk", "108": "Book", "109": "Book talk"},
wgNamespaceIds={"media": -2, "special": -1, "": 0, "talk": 1, "user": 2, "user_talk": 3, "wikipedia": 4, "wikipedia_talk": 5, "file": 6, "file_talk": 7, "mediawiki": 8, "mediawiki_talk": 9, "template": 10, "template_talk": 11, "help": 12, "help_talk": 13, "category": 14, "category_talk": 15, "portal": 100, "portal_talk": 101, "book": 108, "book_talk": 109, "wp": 4, "wt": 5, "image": 6, "image_talk": 7},
wgSiteName="Wikipedia",
wgCategories=["Self-contradictory articles from May 2009", "All self-contradictory articles", "All articles with unsourced statements", "Articles with unsourced statements from April 2010", "Wikipedia external links cleanup", "Wikipedia spam cleanup", "Multivariate statistics", "Singular value decomposition", "Data mining", "Data analysis", "Machine learning"],
wgMWSuggestTemplate="http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest",
wgDBname="enwiki",
wgSearchNamespaces=[0],
wgMWSuggestMessages=["with suggestions", "no suggestions"],
wgRestrictionEdit=[],
wgRestrictionMove=[],
wgFlaggedRevsParams={"tags": {"status": {"levels": 1, "quality": 2, "pristine": 3}}},
wgStableRevisionId=0,
wgWikimediaMobileUrl="http://en.m.wikipedia.org/wiki",
wgCollapsibleNavBucketTest=false,
wgCollapsibleNavForceNewVersion=false,
wgVectorPreferences={"collapsiblenav": {"enable": 1}, "editwarning": {"enable": 1}, "simplesearch": {"enable": 1, "disablesuggest": 0}},
wgVectorEnabledModules={"collapsiblenav": true, "collapsibletabs": true, "editwarning": true, "expandablesearch": false, "footercleanup": false, "simplesearch": true},
wgNotice="",
wgNoticeLocal="";
</SCRIPT>

<SCRIPT 
src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/wikibits.js" 
type=text/javascript></SCRIPT>

<SCRIPT 
src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/jquery.min.js" 
type=text/javascript></SCRIPT>

<SCRIPT 
src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/ajax.js" 
type=text/javascript></SCRIPT>

<SCRIPT 
src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/mwsuggest.js" 
type=text/javascript></SCRIPT>

<SCRIPT 
src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/MobileRedirect.js" 
type=text/javascript></SCRIPT>

<SCRIPT 
src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/plugins.combined.min.js" 
type=text/javascript></SCRIPT>

<SCRIPT 
src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/Vector.combined.min.js" 
type=text/javascript></SCRIPT>

<SCRIPT type=text/javascript>mw.usability.addMessages({'vector-collapsiblenav-more':'More languages','vector-editwarning-warning':'Leaving this page may cause you to lose any changes you have made.\nIf you are logged in, you can disable this warning in the \"Editing\" section of your preferences.','vector-simplesearch-search':'Search','vector-simplesearch-containing':'containing...'});</SCRIPT>

<SCRIPT 
src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/centralnotice.js" 
type=text/javascript></SCRIPT>
<!--[if lt IE 7]><style type="text/css">body{behavior:url("/w/skins-1.5/vector/csshover.htc")}</style><![endif]-->
<SCRIPT 
src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/index.php" 
type=text/javascript></SCRIPT>
</HEAD>
<BODY 
class="mediawiki ltr ns-0 ns-subject page-Principal_component_analysis skin-vector">
<DIV class=noprint id=mw-page-base></DIV>
<DIV class=noprint id=mw-head-base></DIV><!-- content -->
<DIV id=content><A id=top></A>
<DIV id=mw-js-message style="DISPLAY: none"></DIV><!-- sitenotice -->
<DIV id=siteNotice>
<SCRIPT 
type=text/javascript>if (wgNotice != '') document.writeln(wgNotice);</SCRIPT>
</DIV><!-- /sitenotice --><!-- firstHeading -->
<H1 class=firstHeading id=firstHeading>Principal component analysis</H1><!-- /firstHeading --><!-- bodyContent -->
<DIV id=bodyContent><!-- tagline -->
<H3 id=siteSub>From Wikipedia, the free encyclopedia</H3><!-- /tagline --><!-- subtitle -->
<DIV id=contentSub></DIV><!-- /subtitle --><!-- jumpto -->
<DIV id=jump-to-nav>Jump to: <A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#mw-head">navigation</A>, 
<A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#p-search">search</A> 
</DIV><!-- /jumpto --><!-- bodytext -->
<DIV class=dablink>"KLT" redirects here. For the Kanade–Lucas–Tomasi <A 
title="Interest point detection" 
href="http://en.wikipedia.org/wiki/Interest_point_detection">feature</A> tracker 
used in <A title="Computer vision" 
href="http://en.wikipedia.org/wiki/Computer_vision">computer vision</A>, see <A 
title="Kanade–Lucas–Tomasi Feature Tracker" 
href="http://en.wikipedia.org/wiki/Kanade%E2%80%93Lucas%E2%80%93Tomasi_Feature_Tracker">Kanade–Lucas–Tomasi 
Feature Tracker</A>.</DIV>
<TABLE class="metadata plainlinks ambox ambox-content">
  <TBODY>
  <TR>
    <TD class=mbox-image>
      <DIV style="WIDTH: 52px"><IMG height=38 
      alt="Exclamation mark with arrows pointing at each other" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/38px-Ambox_contradict.svg.png" 
      width=38></DIV></TD>
    <TD class=mbox-text>This article or section appears to <B>contradict</B> 
      itself. Please see its <A title="Talk:Principal component analysis" 
      href="http://en.wikipedia.org/wiki/Talk:Principal_component_analysis">talk 
      page</A> for more information. <SMALL><I>(May 
  2009)</I></SMALL></TD></TR></TBODY></TABLE>
<DIV class="thumb tright">
<DIV class=thumbinner style="WIDTH: 222px"><A class=image 
href="http://en.wikipedia.org/wiki/File:GaussianScatterPCA.png"><IMG 
class=thumbimage height=206 alt="" 
src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/220px-GaussianScatterPCA.png" 
width=220></A> 
<DIV class=thumbcaption>
<DIV class=magnify><A class=internal title=Enlarge 
href="http://en.wikipedia.org/wiki/File:GaussianScatterPCA.png"><IMG height=11 
alt="" 
src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/magnify-clip.png" 
width=15></A></DIV>PCA of a <A class=mw-redirect 
title="Multivariate Gaussian distribution" 
href="http://en.wikipedia.org/wiki/Multivariate_Gaussian_distribution">multivariate 
Gaussian distribution</A> centered at (1,3) with a standard deviation of 3 in 
roughly the (0.878, 0.478) direction and of 1 in the orthogonal 
direction.</DIV></DIV></DIV>
<P><B>Principal component analysis (PCA)</B> involves a mathematical procedure 
that transforms a number of possibly correlated variables into a smaller number 
of uncorrelated variables called principal components. The first principal 
component accounts for as much of the variability in the data as possible, and 
each succeeding component accounts for as much of the remaining variability as 
possible. Depending on the field of application, it is also named the discrete 
<B><A title="Karhunen–Loève theorem" 
href="http://en.wikipedia.org/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem">Karhunen–Loève</A> 
transform</B> (<B>K.L.T.</B>), the <B><A title="Harold Hotelling" 
href="http://en.wikipedia.org/wiki/Harold_Hotelling">Hotelling</A> transform</B> 
or <B>proper orthogonal decomposition</B> (<B>POD</B>).</P>
<P>PCA was invented in 1901 by <A title="Karl Pearson" 
href="http://en.wikipedia.org/wiki/Karl_Pearson">Karl Pearson</A>.<SUP 
class=reference id=cite_ref-0><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-0"><SPAN>[</SPAN>1<SPAN>]</SPAN></A></SUP> 
Now it is mostly used as a tool in <A title="Exploratory data analysis" 
href="http://en.wikipedia.org/wiki/Exploratory_data_analysis">exploratory data 
analysis</A> and for making <A title="Predictive modeling" 
href="http://en.wikipedia.org/wiki/Predictive_modeling">predictive models</A>. 
PCA involves the calculation of the <A title="Eigendecomposition of a matrix" 
href="http://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix">eigenvalue 
decomposition</A> of a data <A title="Covariance matrix" 
href="http://en.wikipedia.org/wiki/Covariance_matrix">covariance matrix</A> or 
<A title="Singular value decomposition" 
href="http://en.wikipedia.org/wiki/Singular_value_decomposition">singular value 
decomposition</A> of a <A title="Data matrix" 
href="http://en.wikipedia.org/wiki/Data_matrix">data matrix</A>, usually after 
mean centering the data for each attribute. The results of a PCA are usually 
discussed in terms of component scores and loadings (Shaw, 2003).</P>
<P>PCA is the simplest of the true eigenvector-based multivariate analyses. 
Often, its operation can be thought of as revealing the internal structure of 
the data in a way which best explains the variance in the data. If a 
multivariate dataset is visualised as a set of coordinates in a high-dimensional 
data space (1 axis per variable), PCA supplies the user with a lower-dimensional 
picture, a "shadow" of this object when viewed from its (in some sense) most 
informative viewpoint.</P>
<P>PCA is closely related to <A title="Factor analysis" 
href="http://en.wikipedia.org/wiki/Factor_analysis">factor analysis</A>; indeed, 
some statistical packages deliberately conflate the two techniques. True factor 
analysis makes different assumptions about the underlying structure and solves 
eigenvectors of a slightly different matrix.</P>
<TABLE class=toc id=toc>
  <TBODY>
  <TR>
    <TD>
      <DIV id=toctitle>
      <H2>Contents</H2></DIV>
      <UL>
        <LI class="toclevel-1 tocsection-1"><A 
        href="http://en.wikipedia.org/wiki/Principal_component_analysis#Details"><SPAN 
        class=tocnumber>1</SPAN> <SPAN class=toctext>Details</SPAN></A> 
        <LI class="toclevel-1 tocsection-2"><A 
        href="http://en.wikipedia.org/wiki/Principal_component_analysis#Discussion"><SPAN 
        class=tocnumber>2</SPAN> <SPAN class=toctext>Discussion</SPAN></A> 
        <LI class="toclevel-1 tocsection-3"><A 
        href="http://en.wikipedia.org/wiki/Principal_component_analysis#Table_of_symbols_and_abbreviations"><SPAN 
        class=tocnumber>3</SPAN> <SPAN class=toctext>Table of symbols and 
        abbreviations</SPAN></A> 
        <LI class="toclevel-1 tocsection-4"><A 
        href="http://en.wikipedia.org/wiki/Principal_component_analysis#Properties_and_limitations_of_PCA"><SPAN 
        class=tocnumber>4</SPAN> <SPAN class=toctext>Properties and limitations 
        of PCA</SPAN></A> 
        <LI class="toclevel-1 tocsection-5"><A 
        href="http://en.wikipedia.org/wiki/Principal_component_analysis#Computing_PCA_using_the_covariance_method"><SPAN 
        class=tocnumber>5</SPAN> <SPAN class=toctext>Computing PCA using the 
        covariance method</SPAN></A> 
        <UL>
          <LI class="toclevel-2 tocsection-6"><A 
          href="http://en.wikipedia.org/wiki/Principal_component_analysis#Organize_the_data_set"><SPAN 
          class=tocnumber>5.1</SPAN> <SPAN class=toctext>Organize the data 
          set</SPAN></A> 
          <LI class="toclevel-2 tocsection-7"><A 
          href="http://en.wikipedia.org/wiki/Principal_component_analysis#Calculate_the_empirical_mean"><SPAN 
          class=tocnumber>5.2</SPAN> <SPAN class=toctext>Calculate the empirical 
          mean</SPAN></A> 
          <LI class="toclevel-2 tocsection-8"><A 
          href="http://en.wikipedia.org/wiki/Principal_component_analysis#Calculate_the_deviations_from_the_mean"><SPAN 
          class=tocnumber>5.3</SPAN> <SPAN class=toctext>Calculate the 
          deviations from the mean</SPAN></A> 
          <LI class="toclevel-2 tocsection-9"><A 
          href="http://en.wikipedia.org/wiki/Principal_component_analysis#Find_the_covariance_matrix"><SPAN 
          class=tocnumber>5.4</SPAN> <SPAN class=toctext>Find the covariance 
          matrix</SPAN></A> 
          <LI class="toclevel-2 tocsection-10"><A 
          href="http://en.wikipedia.org/wiki/Principal_component_analysis#Find_the_eigenvectors_and_eigenvalues_of_the_covariance_matrix"><SPAN 
          class=tocnumber>5.5</SPAN> <SPAN class=toctext>Find the eigenvectors 
          and eigenvalues of the covariance matrix</SPAN></A> 
          <LI class="toclevel-2 tocsection-11"><A 
          href="http://en.wikipedia.org/wiki/Principal_component_analysis#Rearrange_the_eigenvectors_and_eigenvalues"><SPAN 
          class=tocnumber>5.6</SPAN> <SPAN class=toctext>Rearrange the 
          eigenvectors and eigenvalues</SPAN></A> 
          <LI class="toclevel-2 tocsection-12"><A 
          href="http://en.wikipedia.org/wiki/Principal_component_analysis#Compute_the_cumulative_energy_content_for_each_eigenvector"><SPAN 
          class=tocnumber>5.7</SPAN> <SPAN class=toctext>Compute the cumulative 
          energy content for each eigenvector</SPAN></A> 
          <LI class="toclevel-2 tocsection-13"><A 
          href="http://en.wikipedia.org/wiki/Principal_component_analysis#Select_a_subset_of_the_eigenvectors_as_basis_vectors"><SPAN 
          class=tocnumber>5.8</SPAN> <SPAN class=toctext>Select a subset of the 
          eigenvectors as basis vectors</SPAN></A> 
          <LI class="toclevel-2 tocsection-14"><A 
          href="http://en.wikipedia.org/wiki/Principal_component_analysis#Convert_the_source_data_to_z-scores"><SPAN 
          class=tocnumber>5.9</SPAN> <SPAN class=toctext>Convert the source data 
          to z-scores</SPAN></A> 
          <LI class="toclevel-2 tocsection-15"><A 
          href="http://en.wikipedia.org/wiki/Principal_component_analysis#Project_the_z-scores_of_the_data_onto_the_new_basis"><SPAN 
          class=tocnumber>5.10</SPAN> <SPAN class=toctext>Project the z-scores 
          of the data onto the new basis</SPAN></A> </LI></UL>
        <LI class="toclevel-1 tocsection-16"><A 
        href="http://en.wikipedia.org/wiki/Principal_component_analysis#Derivation_of_PCA_using_the_covariance_method"><SPAN 
        class=tocnumber>6</SPAN> <SPAN class=toctext>Derivation of PCA using the 
        covariance method</SPAN></A> 
        <LI class="toclevel-1 tocsection-17"><A 
        href="http://en.wikipedia.org/wiki/Principal_component_analysis#Computing_principal_components_with_expectation_maximization"><SPAN 
        class=tocnumber>7</SPAN> <SPAN class=toctext>Computing principal 
        components with expectation maximization</SPAN></A> 
        <UL>
          <LI class="toclevel-2 tocsection-18"><A 
          href="http://en.wikipedia.org/wiki/Principal_component_analysis#The_NIPALS_method"><SPAN 
          class=tocnumber>7.1</SPAN> <SPAN class=toctext>The NIPALS 
          method</SPAN></A> </LI></UL>
        <LI class="toclevel-1 tocsection-19"><A 
        href="http://en.wikipedia.org/wiki/Principal_component_analysis#Relation_between_PCA_and_K-means_clustering"><SPAN 
        class=tocnumber>8</SPAN> <SPAN class=toctext>Relation between PCA and 
        K-means clustering</SPAN></A> 
        <LI class="toclevel-1 tocsection-20"><A 
        href="http://en.wikipedia.org/wiki/Principal_component_analysis#Correspondence_analysis"><SPAN 
        class=tocnumber>9</SPAN> <SPAN class=toctext>Correspondence 
        analysis</SPAN></A> 
        <LI class="toclevel-1 tocsection-21"><A 
        href="http://en.wikipedia.org/wiki/Principal_component_analysis#Generalizations"><SPAN 
        class=tocnumber>10</SPAN> <SPAN class=toctext>Generalizations</SPAN></A> 

        <UL>
          <LI class="toclevel-2 tocsection-22"><A 
          href="http://en.wikipedia.org/wiki/Principal_component_analysis#Nonlinear_generalizations"><SPAN 
          class=tocnumber>10.1</SPAN> <SPAN class=toctext>Nonlinear 
          generalizations</SPAN></A> 
          <LI class="toclevel-2 tocsection-23"><A 
          href="http://en.wikipedia.org/wiki/Principal_component_analysis#Higher_order"><SPAN 
          class=tocnumber>10.2</SPAN> <SPAN class=toctext>Higher 
          order</SPAN></A> 
          <LI class="toclevel-2 tocsection-24"><A 
          href="http://en.wikipedia.org/wiki/Principal_component_analysis#Robustness_-_Weighted_PCA"><SPAN 
          class=tocnumber>10.3</SPAN> <SPAN class=toctext>Robustness - Weighted 
          PCA</SPAN></A> </LI></UL>
        <LI class="toclevel-1 tocsection-25"><A 
        href="http://en.wikipedia.org/wiki/Principal_component_analysis#Software.2Fsource_code"><SPAN 
        class=tocnumber>11</SPAN> <SPAN class=toctext>Software/source 
        code</SPAN></A> 
        <LI class="toclevel-1 tocsection-26"><A 
        href="http://en.wikipedia.org/wiki/Principal_component_analysis#Notes"><SPAN 
        class=tocnumber>12</SPAN> <SPAN class=toctext>Notes</SPAN></A> 
        <LI class="toclevel-1 tocsection-27"><A 
        href="http://en.wikipedia.org/wiki/Principal_component_analysis#See_also"><SPAN 
        class=tocnumber>13</SPAN> <SPAN class=toctext>See also</SPAN></A> 
        <LI class="toclevel-1 tocsection-28"><A 
        href="http://en.wikipedia.org/wiki/Principal_component_analysis#References"><SPAN 
        class=tocnumber>14</SPAN> <SPAN class=toctext>References</SPAN></A> 
        <LI class="toclevel-1 tocsection-29"><A 
        href="http://en.wikipedia.org/wiki/Principal_component_analysis#External_links"><SPAN 
        class=tocnumber>15</SPAN> <SPAN class=toctext>External links</SPAN></A> 
        <UL>
          <LI class="toclevel-2 tocsection-30"><A 
          href="http://en.wikipedia.org/wiki/Principal_component_analysis#Non-technical_introductions"><SPAN 
          class=tocnumber>15.1</SPAN> <SPAN class=toctext>Non-technical 
          introductions</SPAN></A> </LI></UL></LI></UL></TD></TR></TBODY></TABLE>
<SCRIPT type=text/javascript>
//<![CDATA[
if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</SCRIPT>

<H2><SPAN class=editsection>[<A title="Edit section: Details" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=1">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Details>Details</SPAN></H2>
<P>PCA is mathematically defined<SUP class=reference id=cite_ref-1><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-1"><SPAN>[</SPAN>2<SPAN>]</SPAN></A></SUP> 
as an <A class=mw-redirect title="Orthogonal transformation" 
href="http://en.wikipedia.org/wiki/Orthogonal_transformation">orthogonal</A> <A 
class=mw-redirect title="Linear transformation" 
href="http://en.wikipedia.org/wiki/Linear_transformation">linear 
transformation</A> that transforms the data to a new <A 
title="Coordinate system" 
href="http://en.wikipedia.org/wiki/Coordinate_system">coordinate system</A> such 
that the greatest variance by any projection of the data comes to lie on the 
first coordinate (called the first principal component), the second greatest 
variance on the second coordinate, and so on. PCA is theoretically the optimum 
transform for given data in <A title="Least squares" 
href="http://en.wikipedia.org/wiki/Least_squares">least square</A> terms.</P>
<P>For a data <A title="Matrix (mathematics)" 
href="http://en.wikipedia.org/wiki/Matrix_(mathematics)">matrix</A>, 
<B>X<SUP>T</SUP></B>, with zero <A class=mw-redirect title="Empirical mean" 
href="http://en.wikipedia.org/wiki/Empirical_mean">empirical mean</A> (the 
empirical mean of the distribution has been subtracted from the data set), where 
each row represents a different repetition of the experiment, and each column 
gives the results from a particular probe, the PCA transformation is given 
by:</P>
<DL>
  <DD><IMG class=tex alt="\mathbf{Y}^{\rm T}=\mathbf{X}^{\rm T}\mathbf{W}" 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/40774ade43c14dbe14830bca2222513c.png"> 

  <DL>
    <DD><IMG class=tex alt=" = \mathbf{V}\mathbf{\Sigma}^{\rm T} " 
    src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/57957f2e6baf3ed1d39d52cbdb669936.png"> 
    </DD></DL></DD></DL>
<P>where the matrix Σ is an m-by-n diagonal matrix with nonnegative real numbers 
on the diagonal<SUP class=Template-Fact 
title="This claim needs references to reliable sources from April 2010" 
style="WHITE-SPACE: nowrap">[<I><A title="Wikipedia:Citation needed" 
href="http://en.wikipedia.org/wiki/Wikipedia:Citation_needed">citation 
needed</A></I>]</SUP> and <B>W Σ V<SUP>T</SUP></B> is the <A 
title="Singular value decomposition" 
href="http://en.wikipedia.org/wiki/Singular_value_decomposition">singular value 
decomposition</A> (svd) of <B>X</B>.</P>
<P>Given a set of points in <A title="Euclidean space" 
href="http://en.wikipedia.org/wiki/Euclidean_space">Euclidean space</A>, the 
first principal component (the eigenvector with the largest eigenvalue) 
corresponds to a line that passes through the mean and minimizes <A 
title="Least squares" href="http://en.wikipedia.org/wiki/Least_squares">sum 
squared error</A> with those points. The second principal component corresponds 
to the same concept after all correlation with the first principal component has 
been subtracted out from the points. Each eigenvalue indicates the portion of 
the variance that is correlated with each eigenvector. Thus, the sum of all the 
eigenvalues is equal to the sum squared distance of the points with their mean 
divided by the number of dimensions. PCA essentially rotates the set of points 
around their mean in order to align with the first few principal components. 
This moves as much of the variance as possible (using a linear transformation) 
into the first few dimensions. The values in the remaining dimensions, 
therefore, tend to be highly correlated and may be dropped with minimal loss of 
information. PCA is often used in this manner for <A class=mw-redirect 
title="Dimensionality reduction" 
href="http://en.wikipedia.org/wiki/Dimensionality_reduction">dimensionality 
reduction</A>. PCA has the distinction of being the optimal <A class=mw-redirect 
title="Linear transformation" 
href="http://en.wikipedia.org/wiki/Linear_transformation">linear 
transformation</A> for keeping the subspace that has largest variance. This 
advantage, however, comes at the price of greater computational requirement if 
compared, for example, to the <A title="Discrete cosine transform" 
href="http://en.wikipedia.org/wiki/Discrete_cosine_transform">discrete cosine 
transform</A>. <A title="Nonlinear dimensionality reduction" 
href="http://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction">Nonlinear 
dimensionality reduction</A> techniques tend to be more computationally 
demanding than PCA.</P>
<H2><SPAN class=editsection>[<A title="Edit section: Discussion" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=2">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Discussion>Discussion</SPAN></H2>
<P>Mean subtraction (a.k.a. "mean centering") is necessary for performing PCA to 
ensure that the first principal component describes the direction of maximum 
variance. If mean subtraction is not performed, the first principal component 
will instead correspond to the mean of the data. A mean of zero is needed for 
finding a basis that minimizes the <A title="Minimum mean square error" 
href="http://en.wikipedia.org/wiki/Minimum_mean_square_error">mean square 
error</A> of the approximation of the data<SUP class=reference id=cite_ref-2><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-2"><SPAN>[</SPAN>3<SPAN>]</SPAN></A></SUP>.</P>
<P>Assuming zero <A class=mw-redirect title="Empirical mean" 
href="http://en.wikipedia.org/wiki/Empirical_mean">empirical mean</A> (the 
empirical mean of the distribution has been subtracted from the data set), the 
principal component <I>w</I><SUB>1</SUB> of a data set <I>x</I> can be defined 
as:</P>
<DL>
  <DD><IMG class=tex 
  alt="\mathbf{w}_1&#10; = \underset{\Vert \mathbf{w} \Vert = 1}{\operatorname{\arg\,max}}\,\operatorname{Var}\{ \mathbf{w}^T \mathbf{x} \}&#10; = \underset{\Vert \mathbf{w} \Vert = 1}{\operatorname{\arg\,max}}\,E\left\{ \left( \mathbf{w}^T \mathbf{x}\right)^2 \right\}" 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/1dc87c567db8a1451e06ec6241260baa.png"> 
  </DD></DL>
<P>(See <A title="Arg max" href="http://en.wikipedia.org/wiki/Arg_max">arg 
max</A> for the notation.) With the first <I>k</I>&nbsp;−&nbsp;1 components, the 
<I>k</I>th component can be found by subtracting the first <SPAN 
class=texhtml><I>k</I> − 1</SPAN> principal components from <I>x</I>:</P>
<DL>
  <DD><IMG class=tex 
  alt="\mathbf{\hat{x}}_{k - 1}&#10; = \mathbf{x} -&#10; \sum_{i = 1}^{k - 1}&#10; \mathbf{w}_i \mathbf{w}_i^T \mathbf{x}" 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/2f03d7c399d96787d5e3639c08780e4c.png"> 
  </DD></DL>
<P>and by substituting this as the new data set to find a principal component 
in</P>
<DL>
  <DD><IMG class=tex 
  alt="\mathbf{w}_k&#10; = \underset{\Vert \mathbf{w} \Vert = 1}{\operatorname{arg\,max}}\,E\left\{&#10; \left( \mathbf{w}^T \mathbf{\hat{x}}_{k - 1}&#10; \right)^2 \right\}." 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/31cb64b85c4c0504ee2fcc8252451222.png"> 
  </DD></DL>
<P>The Karhunen–Loève transform is therefore equivalent to finding the <A 
title="Singular value decomposition" 
href="http://en.wikipedia.org/wiki/Singular_value_decomposition">singular value 
decomposition</A> of the data matrix <I>X</I>,</P>
<DL>
  <DD><IMG class=tex alt=\mathbf{X}=\mathbf{W}\mathbf{\Sigma}\mathbf{V}^T, 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/f702ff3fe390dc05c7af181fa54dace3.png"> 
  </DD></DL>
<P>and then obtaining the reduced-space data matrix <B>Y</B> by projecting 
<B>X</B> down into the reduced space defined by only the first <I>L</I> singular 
vectors, <B>W<SUB>L</SUB></B>:</P>
<DL>
  <DD><IMG class=tex 
  alt="\mathbf{Y}=\mathbf{W_L}^T\mathbf{X} = \mathbf{\Sigma_L}\mathbf{V_L}^T" 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/af8f42d9aaca5b333df01d982d86fa64.png"> 
  </DD></DL>
<P>The matrix <B>W</B> of singular vectors of <B>X</B> is equivalently the 
matrix <B>W</B> of eigenvectors of the matrix of observed covariances <B>C</B> = 
<B>X X<SUP>T</SUP></B>,</P>
<DL>
  <DD><IMG class=tex 
  alt="\mathbf{X}\mathbf{X}^T = \mathbf{W}\mathbf{\Sigma}\mathbf{\Sigma}^T\mathbf{W}^T" 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/1d0c3ebbdbb9b13a7c68b80aa95386ff.png"> 
  </DD></DL>
<P>The <A class=mw-redirect title=Eigenvectors 
href="http://en.wikipedia.org/wiki/Eigenvectors">eigenvectors</A> with the 
largest <A class=mw-redirect title=Eigenvalues 
href="http://en.wikipedia.org/wiki/Eigenvalues">eigenvalues</A> correspond to 
the dimensions that have the strongest <A class=mw-redirect title=Correlation 
href="http://en.wikipedia.org/wiki/Correlation">correlation</A> in the data set 
(see <A title="Rayleigh quotient" 
href="http://en.wikipedia.org/wiki/Rayleigh_quotient">Rayleigh 
quotient</A>).</P>
<P>PCA is equivalent to <A title="Empirical orthogonal functions" 
href="http://en.wikipedia.org/wiki/Empirical_orthogonal_functions">empirical 
orthogonal functions</A> (EOF), a name which is used in <A title=Meteorology 
href="http://en.wikipedia.org/wiki/Meteorology">meteorology</A>.</P>
<P>An <A class=mw-redirect title=Autoencoder 
href="http://en.wikipedia.org/wiki/Autoencoder">autoencoder</A> <A 
title="Artificial neural network" 
href="http://en.wikipedia.org/wiki/Artificial_neural_network">neural network</A> 
with a linear hidden layer is similar to PCA. Upon convergence, the weight 
vectors of the <I>K</I> neurons in the hidden layer will form a basis for the 
space spanned by the first <I>K</I> principal components. Unlike PCA, this 
technique will not necessarily produce <A class=mw-redirect title=Orthogonal 
href="http://en.wikipedia.org/wiki/Orthogonal">orthogonal</A> vectors.</P>
<P>PCA is a popular primary technique in <A title="Pattern recognition" 
href="http://en.wikipedia.org/wiki/Pattern_recognition">pattern recognition</A>. 
But it is not optimized for class separability<SUP class=reference 
id=cite_ref-3><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-3"><SPAN>[</SPAN>4<SPAN>]</SPAN></A></SUP>. 
An alternative is the <A title="Linear discriminant analysis" 
href="http://en.wikipedia.org/wiki/Linear_discriminant_analysis">linear 
discriminant analysis</A>, which does take this into account.</P>
<H2><SPAN class=editsection>[<A 
title="Edit section: Table of symbols and abbreviations" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=3">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Table_of_symbols_and_abbreviations>Table of symbols 
and abbreviations</SPAN></H2>
<TABLE class=wikitable>
  <TBODY>
  <TR>
    <TH>Symbol</TH>
    <TH>Meaning</TH>
    <TH>Dimensions</TH>
    <TH>Indices</TH></TR>
  <TR>
    <TD><IMG class=tex alt="\mathbf{X} = \{ X[m,n] \}" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/f867e7c98e184b05ed01eedf48d48da5.png"></TD>
    <TD>data matrix, consisting of the set of all data vectors, one vector per 
      column</TD>
    <TD><IMG class=tex alt=" M \times N" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/77a72b608a1beff4bf12b46d0ee0f3bb.png"></TD>
    <TD><IMG class=tex alt=" m = 1 \ldots M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/4d08549f75bda4aa10c97dae0102f4f5.png"><BR><IMG 
      class=tex alt=" n = 1 \ldots N " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/2952b718dcfb00ae3e968eced35aae04.png"></TD></TR>
  <TR>
    <TD><IMG class=tex alt="N \," 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/beed584371120e11bf20723d0f22e52e.png"></TD>
    <TD>the number of column vectors in the data set</TD>
    <TD><IMG class=tex alt="1 \times 1" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/b99a31f00d8eff828fb2b1657efe2f4b.png"></TD>
    <TD><I>scalar</I></TD></TR>
  <TR>
    <TD><IMG class=tex alt="M \," 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/1dd4ab77983ec94cab2e7ff337a739e8.png"></TD>
    <TD>the number of elements in each column vector (dimension)</TD>
    <TD><IMG class=tex alt="1 \times 1" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/b99a31f00d8eff828fb2b1657efe2f4b.png"></TD>
    <TD><I>scalar</I></TD></TR>
  <TR>
    <TD><IMG class=tex alt="L \," 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/43afc2e242876990f6bf778f2a2278d7.png"></TD>
    <TD>the number of dimensions in the dimensionally reduced subspace, <IMG 
      class=tex alt=" 1 \le L \le M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/b84657493e92092fc1a8a364ff62270e.png"></TD>
    <TD><IMG class=tex alt="1 \times 1" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/b99a31f00d8eff828fb2b1657efe2f4b.png"></TD>
    <TD><I>scalar</I></TD></TR>
  <TR>
    <TD><IMG class=tex alt="\mathbf{u} = \{ u[m] \}" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/5c8208a0df7d9bf4da7d597b1870f23d.png"></TD>
    <TD>vector of empirical <A title=Mean 
      href="http://en.wikipedia.org/wiki/Mean">means</A>, one mean for each row 
      <I>m</I> of the data matrix</TD>
    <TD><IMG class=tex alt=" M \times 1" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/082f7fb86728e4c1c5d4c3053756b038.png"></TD>
    <TD><IMG class=tex alt=" m = 1 \ldots M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/4d08549f75bda4aa10c97dae0102f4f5.png"></TD></TR>
  <TR>
    <TD><IMG class=tex alt="\mathbf{s} = \{ s[m] \}" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/c676f10f15d0881db172967871693572.png"></TD>
    <TD>vector of empirical <A title="Standard deviation" 
      href="http://en.wikipedia.org/wiki/Standard_deviation">standard 
      deviations</A>, one standard deviation for each row <I>m</I> of the data 
      matrix</TD>
    <TD><IMG class=tex alt=" M \times 1" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/082f7fb86728e4c1c5d4c3053756b038.png"></TD>
    <TD><IMG class=tex alt=" m = 1 \ldots M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/4d08549f75bda4aa10c97dae0102f4f5.png"></TD></TR>
  <TR>
    <TD><IMG class=tex alt="\mathbf{h} = \{ h[n] \}" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/05c290900324b15ef4123a2293a2c544.png"></TD>
    <TD>vector of all 1's</TD>
    <TD><IMG class=tex alt=" 1 \times N" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/a48c766b7d3907922863031c5140af8b.png"></TD>
    <TD><IMG class=tex alt=" n = 1 \ldots N " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/2952b718dcfb00ae3e968eced35aae04.png"></TD></TR>
  <TR>
    <TD><IMG class=tex alt="\mathbf{B} = \{ B[m,n] \}" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/3b00e80fb5f767ceaba11c14ffbe7ffb.png"></TD>
    <TD><A title=Deviation 
      href="http://en.wikipedia.org/wiki/Deviation">deviations</A> from the mean 
      of each row <I>m</I> of the data matrix</TD>
    <TD><IMG class=tex alt=" M \times N" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/77a72b608a1beff4bf12b46d0ee0f3bb.png"></TD>
    <TD><IMG class=tex alt=" m = 1 \ldots M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/4d08549f75bda4aa10c97dae0102f4f5.png"><BR><IMG 
      class=tex alt=" n = 1 \ldots N " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/2952b718dcfb00ae3e968eced35aae04.png"></TD></TR>
  <TR>
    <TD><IMG class=tex alt="\mathbf{Z} = \{ Z[m,n] \} " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/4e3dbd4fb1b5ef217ba716a1a90d2511.png"></TD>
    <TD><A class=mw-redirect title=Z-score 
      href="http://en.wikipedia.org/wiki/Z-score">z-scores</A>, computed using 
      the mean and standard deviation for each row <I>m</I> of the data 
matrix</TD>
    <TD><IMG class=tex alt=" M \times N" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/77a72b608a1beff4bf12b46d0ee0f3bb.png"></TD>
    <TD><IMG class=tex alt=" m = 1 \ldots M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/4d08549f75bda4aa10c97dae0102f4f5.png"><BR><IMG 
      class=tex alt=" n = 1 \ldots N " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/2952b718dcfb00ae3e968eced35aae04.png"></TD></TR>
  <TR>
    <TD><IMG class=tex alt="\mathbf{C} = \{ C[p,q] \} " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/a2ae59fecd9a30529c7ab8260e534f1a.png"></TD>
    <TD><A title="Covariance matrix" 
      href="http://en.wikipedia.org/wiki/Covariance_matrix">covariance 
    matrix</A></TD>
    <TD><IMG class=tex alt=" M \times M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/627dfe3da8c871a94e9f209e28cc7121.png"></TD>
    <TD><IMG class=tex alt=" p = 1 \ldots M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/ee89e25c5629e9ce5c55751102c22695.png"><BR><IMG 
      class=tex alt=" q = 1 \ldots M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/e71be601212cc102f41ab296a52ea483.png"></TD></TR>
  <TR>
    <TD><IMG class=tex alt="\mathbf{R} = \{ R[p,q] \} " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/042cc0a47cdc31cb96cd0720c0ac51b2.png"></TD>
    <TD><A class=mw-redirect title="Correlation matrix" 
      href="http://en.wikipedia.org/wiki/Correlation_matrix">correlation 
      matrix</A></TD>
    <TD><IMG class=tex alt=" M \times M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/627dfe3da8c871a94e9f209e28cc7121.png"></TD>
    <TD><IMG class=tex alt=" p = 1 \ldots M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/ee89e25c5629e9ce5c55751102c22695.png"><BR><IMG 
      class=tex alt=" q = 1 \ldots M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/e71be601212cc102f41ab296a52ea483.png"></TD></TR>
  <TR>
    <TD><IMG class=tex alt=" \mathbf{V} = \{ V[p,q] \} " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/05f86efc393e12e96b879e1b48e9fb4f.png"></TD>
    <TD>matrix consisting of the set of all <A class=mw-redirect 
      title=Eigenvectors 
      href="http://en.wikipedia.org/wiki/Eigenvectors">eigenvectors</A> of 
      <B>C</B>, one eigenvector per column</TD>
    <TD><IMG class=tex alt=" M \times M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/627dfe3da8c871a94e9f209e28cc7121.png"></TD>
    <TD><IMG class=tex alt=" p = 1 \ldots M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/ee89e25c5629e9ce5c55751102c22695.png"><BR><IMG 
      class=tex alt=" q = 1 \ldots M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/e71be601212cc102f41ab296a52ea483.png"></TD></TR>
  <TR>
    <TD><IMG class=tex alt="\mathbf{D} = \{ D[p,q] \} " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/acdc2b89fb7f17e8ba1abbda6f4f0a01.png"></TD>
    <TD><A title="Diagonal matrix" 
      href="http://en.wikipedia.org/wiki/Diagonal_matrix">diagonal matrix</A> 
      consisting of the set of all <A class=mw-redirect title=Eigenvalues 
      href="http://en.wikipedia.org/wiki/Eigenvalues">eigenvalues</A> of 
      <B>C</B> along its <A class=mw-redirect title="Principal diagonal" 
      href="http://en.wikipedia.org/wiki/Principal_diagonal">principal 
      diagonal</A>, and 0 for all other elements</TD>
    <TD><IMG class=tex alt=" M \times M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/627dfe3da8c871a94e9f209e28cc7121.png"></TD>
    <TD><IMG class=tex alt=" p = 1 \ldots M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/ee89e25c5629e9ce5c55751102c22695.png"><BR><IMG 
      class=tex alt=" q = 1 \ldots M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/e71be601212cc102f41ab296a52ea483.png"></TD></TR>
  <TR>
    <TD><IMG class=tex alt="\mathbf{W} = \{ W[p,q] \} " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/0616329127868d5b6c7a63513a096761.png"></TD>
    <TD>matrix of basis vectors, one vector per column, where each basis 
      vector is one of the <A class=mw-redirect title=Eigenvectors 
      href="http://en.wikipedia.org/wiki/Eigenvectors">eigenvectors</A> of 
      <B>C</B>, and where the vectors in <B>W</B> are a sub-set of those in 
      <B>V</B></TD>
    <TD><IMG class=tex alt=" M \times L" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/eb796e2c1bb1c75bbf23abf6aab2c3fc.png"></TD>
    <TD><IMG class=tex alt=" p = 1 \ldots M " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/ee89e25c5629e9ce5c55751102c22695.png"><BR><IMG 
      class=tex alt=" q = 1 \ldots L" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/b1052cf45aaa22f121ca91086847b53b.png"></TD></TR>
  <TR>
    <TD><IMG class=tex alt="\mathbf{Y} = \{ Y[m,n] \} " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/93ff4f2a52b9c57cb8d4d718c2ec8377.png"></TD>
    <TD>matrix consisting of <I>N</I> column vectors, where each vector is the 
      projection of the corresponding data vector from matrix <B>X</B> onto the 
      basis vectors contained in the columns of matrix <B>W</B>.</TD>
    <TD><IMG class=tex alt=" L \times N" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/7ddb5c4baf504e06e73640cf19bfac01.png"></TD>
    <TD><IMG class=tex alt=" m = 1 \ldots L " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/ac08f0e10ca256f9b346ab33eb2dbf74.png"><BR><IMG 
      class=tex alt=" n = 1 \ldots N" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/2952b718dcfb00ae3e968eced35aae04.png"></TD></TR></TBODY></TABLE>
<H2><SPAN class=editsection>[<A 
title="Edit section: Properties and limitations of PCA" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=4">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Properties_and_limitations_of_PCA>Properties and 
limitations of PCA</SPAN></H2>
<P>PCA is theoretically the optimal linear scheme, in terms of <A 
title="Minimum mean square error" 
href="http://en.wikipedia.org/wiki/Minimum_mean_square_error">least mean square 
error</A>, for compressing a set of high dimensional vectors into a set of lower 
dimensional vectors and then reconstructing the original set. It is a <A 
class=mw-redirect title=Non-parametric 
href="http://en.wikipedia.org/wiki/Non-parametric">non-parametric</A> analysis 
and the answer is unique and independent of any hypothesis about data <A 
title="Probability distribution" 
href="http://en.wikipedia.org/wiki/Probability_distribution">probability 
distribution</A>. However, the latter two properties are regarded as weakness as 
well as strength, in that being non-parametric, no prior knowledge can be 
incorporated and that PCA compressions often incur loss of information.</P>
<P>The applicability of PCA is limited by the assumptions<SUP class=reference 
id=cite_ref-4><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-4"><SPAN>[</SPAN>5<SPAN>]</SPAN></A></SUP> 
made in its derivation. These assumptions are:</P>
<UL>
  <LI>Assumption on linearity </LI></UL>
<DL>
  <DD>We assumed the observed data set to be <A title="Linear combination" 
  href="http://en.wikipedia.org/wiki/Linear_combination">linear combinations</A> 
  of certain basis. Non-linear methods such as <A 
  title="Kernel principal component analysis" 
  href="http://en.wikipedia.org/wiki/Kernel_principal_component_analysis">kernel 
  PCA</A> have been developed without assuming linearity. </DD></DL>
<UL>
  <LI>Assumption on the statistical importance of mean and covariance </LI></UL>
<DL>
  <DD>PCA uses the <A class=mw-redirect title=Eigenvector 
  href="http://en.wikipedia.org/wiki/Eigenvector">eigenvectors</A> of the <A 
  title=Covariance href="http://en.wikipedia.org/wiki/Covariance">covariance</A> 
  matrix and it only finds the independent axes of the data under the Gaussian 
  assumption. For non-Gaussian or multi-modal Gaussian data, PCA simply 
  de-correlates the axes. When PCA is used for clustering, its main limitation 
  is that it does not account for class separability since it makes no use of 
  the class label of the feature vector. There is no guarantee that the 
  directions of maximum variance will contain good features for discrimination. 
  </DD></DL>
<UL>
  <LI>Assumption that large variances have important dynamics </LI></UL>
<DL>
  <DD>PCA simply performs a coordinate rotation that aligns the transformed axes 
  with the directions of maximum variance. It is only when we believe that the 
  observed data has a high signal-to-noise ratio that the principal components 
  with larger variance correspond to interesting dynamics and lower ones 
  correspond to noise. </DD></DL>
<P>Essentially, PCA involves only rotation and scaling. The above assumptions 
are made in order to simplify the algebraic computation on the data set. Some 
other methods have been developed without one or more of these assumptions; 
these are described below.</P>
<H2><SPAN class=editsection>[<A 
title="Edit section: Computing PCA using the covariance method" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=5">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Computing_PCA_using_the_covariance_method>Computing 
PCA using the covariance method</SPAN></H2>
<P>Following is a detailed description of PCA using the covariance method (see 
also <A class="external text" 
href="http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf" 
rel=nofollow>here</A>). The goal is to transform a given data set <B>X</B> of 
dimension <I>M</I> to an alternative data set <B>Y</B> of smaller dimension 
<I>L</I>. Equivalently, we are seeking to find the matrix <B>Y</B>, where 
<B>Y</B> is the <A class=mw-redirect title="Karhunen–Loève transform" 
href="http://en.wikipedia.org/wiki/Karhunen%E2%80%93Lo%C3%A8ve_transform">Karhunen–Loève 
transform</A> (KLT) of matrix <B>X</B>:</P>
<DL>
  <DD><IMG class=tex alt=" \mathbf{Y} = \mathbb{KLT} \{ \mathbf{X} \} " 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/258f9edf465e35e61c82416fd98f0657.png"> 
  </DD></DL>
<H3><SPAN class=editsection>[<A title="Edit section: Organize the data set" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=6">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Organize_the_data_set>Organize the data 
set</SPAN></H3>
<P><B>Suppose</B> you have data comprising a set of observations of <I>M</I> 
variables, and you want to reduce the data so that each observation can be 
described with only <I>L</I> variables, <I>L</I> &lt; <I>M</I>. Suppose further, 
that the data are arranged as a set of <I>N</I> data vectors <IMG class=tex 
alt="\mathbf{x}_1 \ldots \mathbf{x}_N" 
src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/cea8f38160bedce54cb178149a830181.png"> 
with each <IMG class=tex alt="\mathbf{x}_n " 
src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/8e11284eb0afcf6f88e1cdce8b5a26d0.png"> 
representing a single grouped observation of the <I>M</I> variables.</P>
<UL>
  <LI>Write <IMG class=tex alt="\mathbf{x}_1 \ldots \mathbf{x}_N" 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/cea8f38160bedce54cb178149a830181.png"> 
  as column vectors, each of which has <I>M</I> rows. 
  <LI>Place the column vectors into a single matrix <B>X</B> of dimensions 
  <I>M</I> × <I>N</I>. </LI></UL>
<H3><SPAN class=editsection>[<A 
title="Edit section: Calculate the empirical mean" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=7">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Calculate_the_empirical_mean>Calculate the empirical 
mean</SPAN></H3>
<UL>
  <LI>Find the empirical mean along each dimension <I>m</I> = 
  1,&nbsp;...,&nbsp;<I>M</I>. 
  <LI>Place the calculated mean values into an empirical mean vector <B>u</B> of 
  dimensions <I>M</I> × 1. </LI></UL>
<DL>
  <DD>
  <DL>
    <DD><IMG class=tex alt="u[m] = {1 \over N} \sum_{n=1}^N X[m,n] " 
    src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/8957f38829f7b6c873a6248176f16b58.png"> 
    </DD></DL></DD></DL>
<H3><SPAN class=editsection>[<A 
title="Edit section: Calculate the deviations from the mean" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=8">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Calculate_the_deviations_from_the_mean>Calculate the 
deviations from the mean</SPAN></H3>
<P>Mean subtraction is an integral part of the solution towards finding a 
principal component basis that minimizes the mean square error of approximating 
the data<SUP class=reference id=cite_ref-5><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-5"><SPAN>[</SPAN>6<SPAN>]</SPAN></A></SUP>. 
Hence we proceed by centering the data as follows:</P>
<UL>
  <LI>Subtract the empirical mean vector <B>u</B> from each column of the data 
  matrix <B>X</B>. 
  <LI>Store mean-subtracted data in the <I>M</I> × <I>N</I> matrix <B>B</B>. 
  </LI></UL>
<DL>
  <DD>
  <DL>
    <DD><IMG class=tex alt="\mathbf{B} = \mathbf{X} - \mathbf{u}\mathbf{h} " 
    src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/3ec81b924c855d2ae0d7becc1c11bb7d.png"> 

    <DD>where <B>h</B> is a 1 x <I>N</I> row vector of all 1's: </DD></DL></DD></DL>
<DL>
  <DD>
  <DL>
    <DD>
    <DL>
      <DD><IMG class=tex 
      alt="h[n] = 1 \, \qquad \qquad \mathrm{for \ } n = 1 \ldots N " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/17a2338fabe95808174754e820755123.png"> 
      </DD></DL></DD></DL></DD></DL>
<H3><SPAN class=editsection>[<A title="Edit section: Find the covariance matrix" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=9">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Find_the_covariance_matrix>Find the covariance 
matrix</SPAN></H3>
<UL>
  <LI>Find the <I>M</I> × <I>M</I> empirical <A title="Covariance matrix" 
  href="http://en.wikipedia.org/wiki/Covariance_matrix">covariance matrix</A> 
  <B>C</B> from the <A title="Outer product" 
  href="http://en.wikipedia.org/wiki/Outer_product">outer product</A> of matrix 
  <B>B</B> with itself: </LI></UL>
<DL>
  <DD>
  <DL>
    <DD><IMG class=tex 
    alt="\mathbf{C} = \mathbb{ E } \left[ \mathbf{B} \otimes \mathbf{B} \right] = \mathbb{ E } \left[ \mathbf{B} \cdot \mathbf{B}^{*} \right] = { 1 \over N } \sum_{} \mathbf{B} \cdot \mathbf{B}^{*}" 
    src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/7710366946cc46e2032d40a41985c8e5.png"> 

    <DD>where 
    <DL>
      <DD><IMG class=tex alt="\mathbb{E} " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/56bda1c0c911f27b99dec7ff663a12c2.png"> 
      is the <A title="Expected value" 
      href="http://en.wikipedia.org/wiki/Expected_value">expected value</A> 
      operator, 
      <DD><IMG class=tex alt=" \otimes " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/e9dd9013ec300ceba41484dfc2c9a876.png"> 
      is the <A title="Outer product" 
      href="http://en.wikipedia.org/wiki/Outer_product">outer product</A> 
      operator, and 
      <DD><IMG class=tex alt=" * \ " 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/d104d819cb49fadb70952da66a0dc281.png"> 
      is the <A title="Conjugate transpose" 
      href="http://en.wikipedia.org/wiki/Conjugate_transpose">conjugate 
      transpose</A> operator. Note that if B consists entirely of real numbers, 
      which is the case in many applications, the "conjugate transpose" is the 
      same as the regular <A title=Transpose 
      href="http://en.wikipedia.org/wiki/Transpose">transpose</A>. 
  </DD></DL></DD></DL></DD></DL>
<UL>
  <LI>Please note that the information in this section is indeed a bit fuzzy. 
  Outer products apply to vectors, for tensor cases we should apply tensor 
  products, but the covariance matrix in PCA, is a sum of outer products between 
  its sample vectors, indeed it could be represented as B.B*. See the covariance 
  matrix sections on the discussion page for more information. </LI></UL>
<H3><SPAN class=editsection>[<A 
title="Edit section: Find the eigenvectors and eigenvalues of the covariance matrix" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=10">edit</A>]</SPAN> 
<SPAN class=mw-headline 
id=Find_the_eigenvectors_and_eigenvalues_of_the_covariance_matrix>Find the 
eigenvectors and eigenvalues of the covariance matrix</SPAN></H3>
<UL>
  <LI>Compute the matrix <B>V</B> of <A class=mw-redirect title=Eigenvector 
  href="http://en.wikipedia.org/wiki/Eigenvector">eigenvectors</A> which <A 
  title="Diagonalizable matrix" 
  href="http://en.wikipedia.org/wiki/Diagonalizable_matrix">diagonalizes</A> the 
  covariance matrix <B>C</B>: </LI></UL>
<DL>
  <DD>
  <DL>
    <DD><IMG class=tex alt="\mathbf{V}^{-1} \mathbf{C} \mathbf{V} = \mathbf{D} " 
    src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/3cb46a0530b1320c6a7bc3bd72b7ed3f.png"> 
    </DD></DL></DD></DL>
<DL>
  <DD>where <B>D</B> is the <A title="Diagonal matrix" 
  href="http://en.wikipedia.org/wiki/Diagonal_matrix">diagonal matrix</A> of <A 
  class=mw-redirect title=Eigenvalue 
  href="http://en.wikipedia.org/wiki/Eigenvalue">eigenvalues</A> of <B>C</B>. 
  This step will typically involve the use of a computer-based algorithm for 
  computing eigenvectors and eigenvalues. These algorithms are readily available 
  as sub-components of most <A title="Matrix algebra" 
  href="http://en.wikipedia.org/wiki/Matrix_algebra">matrix algebra</A> systems, 
  such as <A title=MATLAB 
  href="http://en.wikipedia.org/wiki/MATLAB">MATLAB</A><SUP class=reference 
  id=cite_ref-6><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-6"><SPAN>[</SPAN>7<SPAN>]</SPAN></A></SUP><SUP 
  class=reference id=cite_ref-7><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-7"><SPAN>[</SPAN>8<SPAN>]</SPAN></A></SUP>, 
  <A title=Mathematica 
  href="http://en.wikipedia.org/wiki/Mathematica">Mathematica</A><SUP 
  class=reference id=cite_ref-8><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-8"><SPAN>[</SPAN>9<SPAN>]</SPAN></A></SUP>, 
  <A title=SciPy href="http://en.wikipedia.org/wiki/SciPy">SciPy</A>, <A 
  title=IDL href="http://en.wikipedia.org/wiki/IDL">IDL</A>(<A class=mw-redirect 
  title="Interactive Data Language" 
  href="http://en.wikipedia.org/wiki/Interactive_Data_Language">Interactive Data 
  Language</A>), or <A title="GNU Octave" 
  href="http://en.wikipedia.org/wiki/GNU_Octave">GNU Octave</A> as well as <A 
  title=OpenCV href="http://en.wikipedia.org/wiki/OpenCV">OpenCV</A>. </DD></DL>
<UL>
  <LI>Matrix <B>D</B> will take the form of an <I>M</I> × <I>M</I> <A 
  title="Diagonal matrix" 
  href="http://en.wikipedia.org/wiki/Diagonal_matrix">diagonal matrix</A>, where 
  </LI></UL>
<DL>
  <DD>
  <DL>
    <DD><IMG class=tex 
    alt="D[p,q] = \lambda_m \qquad \mathrm{for} \qquad p = q = m" 
    src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/e64dd570e3822288e9e85e0e21bda469.png"> 
    </DD></DL></DD></DL>
<DL>
  <DD>is the <I>m</I>th eigenvalue of the covariance matrix <B>C</B>, and 
</DD></DL>
<DL>
  <DD>
  <DL>
    <DD><IMG class=tex alt="D[p,q] = 0 \qquad \mathrm{for} \qquad p \ne q." 
    src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/b7f35034926231465eb6593364e75729.png"> 
    </DD></DL></DD></DL>
<UL>
  <LI>Matrix <B>V</B>, also of dimension <I>M</I> × <I>M</I>, contains <I>M</I> 
  column vectors, each of length <I>M</I>, which represent the <I>M</I> 
  eigenvectors of the covariance matrix <B>C</B>. 
  <LI>The eigenvalues and eigenvectors are ordered and paired. The <I>m</I>th 
  eigenvalue corresponds to the <I>m</I>th eigenvector. </LI></UL>
<H3><SPAN class=editsection>[<A 
title="Edit section: Rearrange the eigenvectors and eigenvalues" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=11">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Rearrange_the_eigenvectors_and_eigenvalues>Rearrange 
the eigenvectors and eigenvalues</SPAN></H3>
<UL>
  <LI>Sort the columns of the eigenvector matrix <B>V</B> and eigenvalue matrix 
  <B>D</B> in order of <I>decreasing</I> eigenvalue. 
  <LI>Make sure to maintain the correct pairings between the columns in each 
  matrix. </LI></UL>
<H3><SPAN class=editsection>[<A 
title="Edit section: Compute the cumulative energy content for each eigenvector" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=12">edit</A>]</SPAN> 
<SPAN class=mw-headline 
id=Compute_the_cumulative_energy_content_for_each_eigenvector>Compute the 
cumulative energy content for each eigenvector</SPAN></H3>
<UL>
  <LI>The eigenvalues represent the distribution of the source data's energy 
  among each of the eigenvectors, where the eigenvectors form a <A 
  title="Basis (linear algebra)" 
  href="http://en.wikipedia.org/wiki/Basis_(linear_algebra)">basis</A> for the 
  data. The cumulative energy content <I>g</I> for the <I>m</I>th eigenvector is 
  the sum of the energy content across all of the eigenvalues from 1 through 
  <I>m</I>: </LI></UL>
<DL>
  <DD>
  <DL>
    <DD><IMG class=tex 
    alt="g[m] = \sum_{q=1}^m D[q,q] \qquad \mathrm{for} \qquad m = 1,\dots,M " 
    src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/d197580a393c78139fc14d101901cc62.png"> 
    </DD></DL></DD></DL>
<H3><SPAN class=editsection>[<A 
title="Edit section: Select a subset of the eigenvectors as basis vectors" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=13">edit</A>]</SPAN> 
<SPAN class=mw-headline 
id=Select_a_subset_of_the_eigenvectors_as_basis_vectors>Select a subset of the 
eigenvectors as basis vectors</SPAN></H3>
<UL>
  <LI>Save the first <I>L</I> columns of <B>V</B> as the <I>M</I> × <I>L</I> 
  matrix <B>W</B>: </LI></UL>
<DL>
  <DD>
  <DL>
    <DD><IMG class=tex 
    alt=" W[p,q] = V[p,q] \qquad \mathrm{for} \qquad p = 1,\dots,M \qquad q = 1,\dots,L " 
    src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/5a77ea6fd7f837c31fc3726421950b56.png"> 
    </DD></DL></DD></DL>
<DL>
  <DD>where </DD></DL>
<DL>
  <DD>
  <DL>
    <DD><IMG class=tex alt="1 \leq L \leq M." 
    src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/61b8fda08d84949e36a83dd6dd7c1d76.png"> 
    </DD></DL></DD></DL>
<UL>
  <LI>Use the vector <B>g</B> as a guide in choosing an appropriate value for 
  <I>L</I>. The goal is to choose a value of <I>L</I> as small as possible while 
  achieving a reasonably high value of <I>g</I> on a percentage basis. For 
  example, you may want to choose <I>L</I> so that the cumulative energy 
  <I>g</I> is above a certain threshold, like 90 percent. In this case, choose 
  the smallest value of <I>L</I> such that </LI></UL>
<DL>
  <DD>
  <DL>
    <DD><IMG class=tex alt=" g[m=L] \ge 90%\, " 
    src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/f534234e2b3f080c025e414e8438ebf1.png"> 
    </DD></DL></DD></DL>
<H3><SPAN class=editsection>[<A 
title="Edit section: Convert the source data to z-scores" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=14">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Convert_the_source_data_to_z-scores>Convert the 
source data to z-scores</SPAN></H3>
<UL>
  <LI>Create an <I>M</I> × 1 empirical standard deviation vector <B>s</B> from 
  the square root of each element along the main diagonal of the covariance 
  matrix <B>C</B>: </LI></UL>
<DL>
  <DD>
  <DL>
    <DD><IMG class=tex 
    alt=" \mathbf{s} = \{ s[m] \} = \sqrt{C[p,q]} \qquad \mathrm{for \ } p = q = m = 1 \ldots M " 
    src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/b7721571befb1ed62f64d0a3fa49f426.png"> 
    </DD></DL></DD></DL>
<UL>
  <LI>Calculate the <I>M</I> × <I>N</I> <A title="Standard score" 
  href="http://en.wikipedia.org/wiki/Standard_score">z-score</A> matrix: 
</LI></UL>
<DL>
  <DD>
  <DL>
    <DD><IMG class=tex 
    alt=" \mathbf{Z} = { \mathbf{B} \over \mathbf{s} \cdot \mathbf{h} } " 
    src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/4dda8c62ae717e6c2a16449e3e2747c6.png"> 
    (divide element-by-element) </DD></DL></DD></DL>
<UL>
  <LI>Note: While this step is useful for various applications as it normalizes 
  the data set with respect to its variance, it is not integral part of PCA/KLT! 
  </LI></UL>
<H3><SPAN class=editsection>[<A 
title="Edit section: Project the z-scores of the data onto the new basis" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=15">edit</A>]</SPAN> 
<SPAN class=mw-headline 
id=Project_the_z-scores_of_the_data_onto_the_new_basis>Project the z-scores of 
the data onto the new basis</SPAN></H3>
<UL>
  <LI>The projected vectors are the columns of the matrix </LI></UL>
<DL>
  <DD>
  <DL>
    <DD><IMG class=tex 
    alt=" \mathbf{Y} = \mathbf{W}^* \cdot \mathbf{Z} = \mathbb{KLT} \{ \mathbf{X} \}." 
    src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/c1d186cd6743d216ae15913a17c53c9e.png"> 
    </DD></DL></DD></DL>
<UL>
  <LI><B>W*</B> is the conjugate transpose of the eigenvector matrix. 
  <LI>The columns of matrix <B>Y</B> represent the <A class=mw-redirect 
  title="Karhunen–Loeve transform" 
  href="http://en.wikipedia.org/wiki/Karhunen%E2%80%93Loeve_transform">Karhunen–Loeve 
  transforms</A> (KLT) of the data vectors in the columns of 
  matrix&nbsp;<B>X</B>. </LI></UL>
<H2><SPAN class=editsection>[<A 
title="Edit section: Derivation of PCA using the covariance method" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=16">edit</A>]</SPAN> 
<SPAN class=mw-headline 
id=Derivation_of_PCA_using_the_covariance_method>Derivation of PCA using the 
covariance method</SPAN></H2>
<P>Let <B>X</B> be a <I>d</I>-dimensional random vector expressed as column 
vector. Without loss of generality, assume <B>X</B> has zero mean. We want to 
find a <IMG class=tex alt="d \times d" 
src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/c0d8b4b800fcd5f789307e7b57bd6c2f.png"> 
<A title="Orthonormal basis" 
href="http://en.wikipedia.org/wiki/Orthonormal_basis">orthonormal transformation 
matrix</A> <B>P</B> such that</P>
<DL>
  <DD><IMG class=tex alt="\mathbf{Y} = \mathbf{P}^\top \mathbf{X}" 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/7872a5de26a89bb859486e092eba45f4.png"> 
  </DD></DL>
<P>with the constraint that</P>
<DL>
  <DD><IMG class=tex alt=\operatorname{cov}(\mathbf{Y}) 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/0daf46117b30fb9fad55fcaf86220abe.png"> 
  is a <A title="Diagonal matrix" 
  href="http://en.wikipedia.org/wiki/Diagonal_matrix">diagonal matrix</A> and 
  <IMG class=tex alt="\mathbf{P}^{-1} = \mathbf{P}^\top." 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/1a6dff08b94c7ce5532cb1a7bf724c5c.png"> 
  </DD></DL>
<P>By substitution, and matrix algebra, we obtain:</P>
<DL>
  <DD><IMG class=tex 
  alt="&#10;\begin{align}&#10;\operatorname{cov}(\mathbf{Y}) &amp;= \mathbb{E}[ \mathbf{Y} \mathbf{Y}^\top]\\&#10;\ &amp;= \mathbb{E}[( \mathbf{P}^\top \mathbf{X} ) ( \mathbf{P}^\top \mathbf{X} )^\top]\\&#10;\ &amp;= \mathbb{E}[(\mathbf{P}^\top \mathbf{X}) (\mathbf{X}^\top \mathbf{P})] \\&#10;\ &amp;= \mathbf{P}^\top \mathbb{E}[\mathbf{X} \mathbf{X}^\top] \mathbf{P} \\&#10;\ &amp;= \mathbf{P}^\top \operatorname{cov}(\mathbf{X}) \mathbf{P}&#10;\end{align}&#10;" 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/a9cb343c4eb68cd1007fbe955b96d330.png"> 
  </DD></DL>
<P>We now have:</P>
<DL>
  <DD><IMG class=tex 
  alt="&#10;\begin{align}&#10;\mathbf{P}\operatorname{cov}(\mathbf{Y}) &amp;= \mathbf{P} \mathbf{P}^\top \operatorname{cov}(\mathbf{X}) \mathbf{P}\\&#10;\ &amp;= \operatorname{cov}(\mathbf{X}) \mathbf{P}\\&#10;\end{align}&#10;" 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/cf00b1266ddaa209b9c611e2f0c2e95e.png"> 
  </DD></DL>
<P>Rewrite <B>P</B> as d <IMG class=tex alt="d \times 1" 
src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/0ea4bb5d9ea9d5b56be8e2a1007cf73e.png"> 
column vectors, so</P>
<DL>
  <DD><IMG class=tex alt="\mathbf{P} = [P_1, P_2, \ldots, P_d]" 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/c1703cd93c71377fe45dab83bcb23dc4.png"> 
  </DD></DL>
<P>and <IMG class=tex alt=\operatorname{cov}(\mathbf{Y}) 
src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/0daf46117b30fb9fad55fcaf86220abe.png"> 
as:</P>
<DL>
  <DD><IMG class=tex 
  alt="&#10;\begin{bmatrix}&#10;\lambda_1 &amp; \cdots &amp; 0 \\&#10;\vdots &amp; \ddots &amp; \vdots \\&#10;0 &amp; \cdots &amp; \lambda_d&#10;\end{bmatrix}.&#10;" 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/202fb379b1b540978fda04d18ecf05cd.png"> 
  </DD></DL>
<P>Substituting into equation above, we obtain:</P>
<DL>
  <DD><IMG class=tex 
  alt="[\lambda_1 P_1, \lambda_2 P_2, \ldots, \lambda_d P_d] =&#10;[\operatorname{cov}(\mathbf{X})P_1, \operatorname{cov}(\mathbf{X})P_2,&#10;\ldots, \operatorname{cov}(\mathbf{X})P_d]." 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/365d7a86dbcc0a19ec18400fc11f417a.png"> 
  </DD></DL>
<P>Notice that in <IMG class=tex 
alt="\lambda_i P_i = \operatorname{cov}(\mathbf{X})P_i" 
src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/a0057fd51ca1fda7ea2d851efdc4bf6b.png">, 
<I>P</I><SUB><I>i</I></SUB> is an <A class=mw-redirect title=Eigenvector 
href="http://en.wikipedia.org/wiki/Eigenvector">eigenvector</A> of the 
covariance matrix of <B>X</B>. Therefore, by finding the eigenvectors of the 
covariance matrix of <B>X</B>, we find a projection matrix <B>P</B> that 
satisfies the original constraints.</P>
<H2><SPAN class=editsection>[<A 
title="Edit section: Computing principal components with expectation maximization" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=17">edit</A>]</SPAN> 
<SPAN class=mw-headline 
id=Computing_principal_components_with_expectation_maximization>Computing 
principal components with expectation maximization</SPAN></H2>
<P>In practical implementations especially with high dimensional data, the 
covariance method is rarely used because it is not efficient. <A 
class=mw-redirect title="Expectation maximization" 
href="http://en.wikipedia.org/wiki/Expectation_maximization">Expectation 
maximization</A> is one way to compute principal components efficiently<SUP 
class=reference id=cite_ref-roweis_9-0><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-roweis-9"><SPAN>[</SPAN>10<SPAN>]</SPAN></A></SUP>. 
The following pseudo-code computes the first principal component of a data 
matrix, <B>X<SUP>T</SUP></B>, with zero mean, without ever computing its 
covariance<SUP class=reference id=cite_ref-roweis_9-1><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-roweis-9"><SPAN>[</SPAN>10<SPAN>]</SPAN></A></SUP>,</P><PRE><IMG class=tex alt="\mathbf{p} =" src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/497dd0621acf4da56d5ee548e1c29ec8.png"> a random vector
do <I>c</I> times:
      <IMG class=tex alt="\mathbf{t} = 0" src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/e47bfd34d26325e317814328161aced2.png">
      for each row <IMG class=tex alt="\mathbf{x} \in \mathbf{X^T}" src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/51058486d198d5d5d0561b289b15f33e.png">
            <IMG class=tex alt="\mathbf{t} = \mathbf{t} + (\mathbf{x} \cdot \mathbf{p})\mathbf{x}" src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/8248ceb5afa7a1eeda61ead9f69884d8.png">
      <IMG class=tex alt="\mathbf{p} = \frac{\mathbf{t}}{|\mathbf{t}|}" src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/f9c190ac9cf3b11c6aacaaf522320404.png">
return <IMG class=tex alt=\mathbf{p} src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/0dad9ef8dd232ad6f3ee4649e5eb1573.png">
</PRE>
<P>The algorithm is nothing but the <A title="Power iteration" 
href="http://en.wikipedia.org/wiki/Power_iteration">power iteration</A> applied 
to the covariance matrix expressed as a sum of outer products(thereby avoiding 
direct calculation of the matrix itself, and thus requiring less computation). 
<B>p</B> will typically converge to the first principal component of 
<B>X<SUP>T</SUP></B> within a small number of iterations, <I>c</I>. (The 
magnitude of <B>t</B> will be larger after each iteration. Convergence can be 
detected when it increases by an amount too small for the precision of the 
machine.)</P>
<P>Subsequent principal components can be computed by subtracting component 
<B>p</B> from <B>X<SUP>T</SUP></B> (see <A class=mw-redirect title=Gram–Schmidt 
href="http://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt">Gram–Schmidt</A>) and 
then repeating this algorithm to find the next principal component. However this 
simple approach is not numerically stable if more than a small number of 
principal components are required, because imprecisions in the calculations will 
additively affect the estimates of subsequent principal components. More 
advanced methods build on this basic idea, as with the closely related <A 
title="Lanczos algorithm" 
href="http://en.wikipedia.org/wiki/Lanczos_algorithm">Lanczos algorithm</A>.</P>
<P>One way to compute the eigenvalue that corresponds with each principal 
component is to measure the difference in sum-squared-distance between the rows 
and the mean, before and after subtracting out the principal component. The 
eigenvalue that corresponds with the component that was removed is equal to this 
difference divided by the number of dimensions (columns).</P>
<H3><SPAN class=editsection>[<A title="Edit section: The NIPALS method" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=18">edit</A>]</SPAN> 
<SPAN class=mw-headline id=The_NIPALS_method>The NIPALS method</SPAN></H3>
<DIV class="rellink relarticle mainarticle">Main article: <A 
title="Non-linear iterative partial least squares" 
href="http://en.wikipedia.org/wiki/Non-linear_iterative_partial_least_squares">Non-linear 
iterative partial least squares</A></DIV>
<P>For very high-dimensional datasets, such as those generated in the 'omics 
sciences (e.g., <A title=Genomics 
href="http://en.wikipedia.org/wiki/Genomics">genomics</A>, <A title=Metabolomics 
href="http://en.wikipedia.org/wiki/Metabolomics">metabolomics</A>) it is usually 
only necessary to compute the first few PCs. The <A 
title="Non-linear iterative partial least squares" 
href="http://en.wikipedia.org/wiki/Non-linear_iterative_partial_least_squares">non-linear 
iterative partial least squares</A> (NIPALS) algorithm calculates 
<B>t<SUB>1</SUB></B> and <B>p<SUB>1</SUB>'</B> from <B>X</B>. The outer product, 
<B>t<SUB>1</SUB>p<SUB>1</SUB>'</B> can then be subtracted from <B>X</B> leaving 
the residual matrix <B>E<SUB>1</SUB></B>. This can be then used to calculate 
subsequent PCs.<SUP class=reference id=cite_ref-10><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-10"><SPAN>[</SPAN>11<SPAN>]</SPAN></A></SUP> 
This results in a dramatic reduction in computational time since calculation of 
the covariance matrix is avoided.</P>
<H2><SPAN class=editsection>[<A 
title="Edit section: Relation between PCA and K-means clustering" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=19">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Relation_between_PCA_and_K-means_clustering>Relation 
between PCA and <I>K</I>-means clustering</SPAN></H2>
<P>It has been shown recently (2007) <SUP class=reference id=cite_ref-11><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-11"><SPAN>[</SPAN>12<SPAN>]</SPAN></A></SUP> 
<SUP class=reference id=cite_ref-12><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-12"><SPAN>[</SPAN>13<SPAN>]</SPAN></A></SUP> 
that the relaxed solution of <A title="K-means clustering" 
href="http://en.wikipedia.org/wiki/K-means_clustering">K-means clustering</A>, 
specified by the cluster indicators, is given by the PCA principal components, 
and the PCA subspace spanned by the principal directions is identical to the 
cluster centroid subspace specified by the between-class <A 
title="Scatter matrix" 
href="http://en.wikipedia.org/wiki/Scatter_matrix">scatter matrix</A>. Thus PCA 
automatically projects to the subspace where the global solution of K-means 
clustering lie, and thus facilitate K-means clustering to find near-optimal 
solutions.</P>
<H2><SPAN class=editsection>[<A title="Edit section: Correspondence analysis" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=20">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Correspondence_analysis>Correspondence 
analysis</SPAN></H2>
<P><B><A title="Correspondence analysis" 
href="http://en.wikipedia.org/wiki/Correspondence_analysis">Correspondence 
analysis</A></B> (CA) was developed by <A title="Jean-Paul Benzécri" 
href="http://en.wikipedia.org/wiki/Jean-Paul_Benz%C3%A9cri">Jean-Paul 
Benzécri</A><SUP class=reference id=cite_ref-13><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-13"><SPAN>[</SPAN>14<SPAN>]</SPAN></A></SUP> 
and is conceptually similar to PCA, but scales the data (which should be 
non-negative) so that rows and columns are treated equivalently. It is 
traditionally applied to <A class=mw-redirect title="Contingency tables" 
href="http://en.wikipedia.org/wiki/Contingency_tables">contingency tables</A>. 
CA decomposes the <A class=mw-redirect title=Chi-square 
href="http://en.wikipedia.org/wiki/Chi-square">chi-square</A> statistic 
associated to this table into orthogonal factors<SUP class=reference 
id=cite_ref-14><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-14"><SPAN>[</SPAN>15<SPAN>]</SPAN></A></SUP>. 
Because CA is a descriptive technique, it can be applied to tables for which the 
chi-square statistic is appropriate or not. Several variants of CA are available 
including <A title="Detrended correspondence analysis" 
href="http://en.wikipedia.org/wiki/Detrended_correspondence_analysis">detrended 
correspondence analysis</A> and <A class=new 
title="Canonical correspondence analysis (page does not exist)" 
href="http://en.wikipedia.org/w/index.php?title=Canonical_correspondence_analysis&amp;action=edit&amp;redlink=1">canonical 
correspondence analysis</A>. One special extension is <A 
title="Multiple correspondence analysis" 
href="http://en.wikipedia.org/wiki/Multiple_correspondence_analysis">Multiple 
correspondence analysis</A>, which may be seen as the counterpart of principal 
component analysis for categorical data.<SUP class=reference id=cite_ref-15><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-15"><SPAN>[</SPAN>16<SPAN>]</SPAN></A></SUP>.</P>
<H2><SPAN class=editsection>[<A title="Edit section: Generalizations" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=21">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Generalizations>Generalizations</SPAN></H2>
<H3><SPAN class=editsection>[<A title="Edit section: Nonlinear generalizations" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=22">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Nonlinear_generalizations>Nonlinear 
generalizations</SPAN></H3>
<P>Most of the modern methods for <A title="Nonlinear dimensionality reduction" 
href="http://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction">nonlinear 
dimensionality reduction</A> find their theoretical and algorithmic roots in PCA 
or K-means. Pearson's original idea was to take a straight line (or plane) which 
will be "the best fit" to a set of data points. <B>Principal <A title=Curve 
href="http://en.wikipedia.org/wiki/Curve">curves</A> and <A title=Manifold 
href="http://en.wikipedia.org/wiki/Manifold">manifolds</A></B><SUP 
class=reference id=cite_ref-16><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-16"><SPAN>[</SPAN>17<SPAN>]</SPAN></A></SUP> 
give the natural geometric framework for PCA generalization and extend the 
geometric interpretation of PCA by explicitly constructing an embedded manifold 
for data <A title=Approximation 
href="http://en.wikipedia.org/wiki/Approximation">approximation</A>, and by 
encoding using standard geometric <A title="Projection (mathematics)" 
href="http://en.wikipedia.org/wiki/Projection_(mathematics)">projection</A> onto 
the manifold. See also <A title="Principal geodesic analysis" 
href="http://en.wikipedia.org/wiki/Principal_geodesic_analysis">principal 
geodesic analysis</A>.</P>
<H3><SPAN class=editsection>[<A title="Edit section: Higher order" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=23">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Higher_order>Higher order</SPAN></H3>
<P><I>N</I>-way principal component analysis may be performed with models such 
as <A title="Tucker decomposition" 
href="http://en.wikipedia.org/wiki/Tucker_decomposition">Tucker 
decomposition</A>, <A title=PARAFAC 
href="http://en.wikipedia.org/wiki/PARAFAC">PARAFAC</A>, multiple factor 
analysis, co-inertia analysis, STATIS, and DISTATIS.</P>
<H3><SPAN class=editsection>[<A title="Edit section: Robustness - Weighted PCA" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=24">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Robustness_-_Weighted_PCA>Robustness - Weighted 
PCA</SPAN></H3>
<P>While PCA finds the mathematically optimal method (as in minimizing the 
squared error), it is sensitive to <A title=Outlier 
href="http://en.wikipedia.org/wiki/Outlier">outliers</A> in the data that 
produce large errors PCA tries to avoid. It therefore is common practise to 
remove outliers before computing PCA. However not in every context, outliers can 
easily be identified. For example in <A title="Data mining" 
href="http://en.wikipedia.org/wiki/Data_mining">data mining</A> algorithms like 
<A title="Correlation clustering" 
href="http://en.wikipedia.org/wiki/Correlation_clustering">correlation 
clustering</A>, the assignment of points to clusters and outliers is not known 
beforehand. A recently proposed generalization of PCA <SUP class=reference 
id=cite_ref-17><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_note-17"><SPAN>[</SPAN>18<SPAN>]</SPAN></A></SUP> 
based on a <B>Weighted PCA</B> increases robustness by assigning different 
weights to data objects based on their estimated relevancy.</P>
<H2><SPAN class=editsection>[<A title="Edit section: Software/source code" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=25">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Software.2Fsource_code>Software/source 
code</SPAN></H2>
<UL>
  <LI><A class="external text" 
  href="http://www.mdp.edu.ar/psicologia/vista/vista.htm" rel=nofollow>"ViSta: 
  The Visual Statistics System"</A> a free software that provides principal 
  components analysis, simple and multiple correspondence analysis. 
  <LI><A class="external text" href="http://www.coloritto.com/" 
  rel=nofollow>"Spectramap"</A> is software to create a <A title=Biplot 
  href="http://en.wikipedia.org/wiki/Biplot">biplot</A> using principal 
  components analysis, correspondence analysis or spectral map analysis. 
  <LI><A title="The Unscrambler" 
  href="http://en.wikipedia.org/wiki/The_Unscrambler">The Unscrambler</A> is a 
  multivariate analysis software enabling Principal Component Analysis (PCA) 
  with PCA Projection. 
  <LI><A class="external text" 
  href="http://sourceforge.net/projects/opencvlibrary/" rel=nofollow>Computer 
  Vision Library</A> 
  <LI><A class="external text" 
  href="http://astro.u-strasbg.fr/~fmurtagh/mda-sw/" rel=nofollow>Multivariate 
  Data Analysis Software</A> 
  <LI>In the <A title=MATLAB 
  href="http://en.wikipedia.org/wiki/MATLAB">MATLAB</A> Statistics Toolbox, the 
  functions <CODE>princomp</CODE> and <CODE>wmspca</CODE> give the principal 
  components, while the function <CODE>pcares</CODE> gives the residuals and 
  reconstructed matrix for a low-rank PCA approximation. Here is a link to a <A 
  title=MATLAB href="http://en.wikipedia.org/wiki/MATLAB">MATLAB</A> 
  implementation of PCA <A class="external text" 
  href="http://www.utdallas.edu/~herve/abdi-PCA4Wiley.zip" 
  rel=nofollow><CODE>PcaPress</CODE></A> . 
  <LI><A title=NMath href="http://en.wikipedia.org/wiki/NMath">NMath</A>, a 
  numerical library containing PCA for the <A title=".NET Framework" 
  href="http://en.wikipedia.org/wiki/.NET_Framework">.NET Framework</A>. 
  <LI>in <A title="GNU Octave" 
  href="http://en.wikipedia.org/wiki/GNU_Octave">Octave</A>, the free software 
  equivalent to <A title=MATLAB 
  href="http://en.wikipedia.org/wiki/MATLAB">MATLAB</A>, the function <A 
  class="external text" href="http://octave.sourceforge.net/doc/statistics.html" 
  rel=nofollow><CODE>princomp</CODE></A> gives the principal component 
  <LI>in the open source statistical package <A title="R (programming language)" 
  href="http://en.wikipedia.org/wiki/R_(programming_language)">R</A>, the 
  functions <A class="external text" 
  href="http://stat.ethz.ch/R-manual/R-patched/library/stats/html/princomp.html" 
  rel=nofollow><CODE>princomp</CODE></A> and <A class="external text" 
  href="http://stat.ethz.ch/R-manual/R-patched/library/stats/html/prcomp.html" 
  rel=nofollow><CODE>prcomp</CODE></A> can be used for principal component 
  analysis; <CODE>prcomp</CODE> uses <A title="Singular value decomposition" 
  href="http://en.wikipedia.org/wiki/Singular_value_decomposition">singular 
  value decomposition</A> which generally gives better numerical accuracy. 
  Recently there has been an explosion in implementations of principal component 
  analysis in various R packages, generally in packages for specific purposes. 
  There is no meaning in giving a complete list here, see <A 
  class="external autonumber" 
  href="http://cran.r-project.org/web/views/Multivariate.html" 
  rel=nofollow>[1]</A>. 
  <LI>In <I>XLMiner</I>, the Principles Component tab can be used for principal 
  component analysis. 
  <LI>In <A title="IDL (programming language)" 
  href="http://en.wikipedia.org/wiki/IDL_(programming_language)">IDL</A>, the 
  principal components can be calculated using the function <CODE>pcomp</CODE>. 
  <LI><A title="Weka (machine learning)" 
  href="http://en.wikipedia.org/wiki/Weka_(machine_learning)">Weka</A> computes 
  principal components (<A class="external text" 
  href="http://weka.sourceforge.net/doc/weka/attributeSelection/PrincipalComponents.html" 
  rel=nofollow>javadoc</A>). 
  <LI><A class="external text" href="http://www.qlucore.com/" 
  rel=nofollow>Software for analyzing multivariate data with instant response 
  using PCA</A> </LI></UL>
<H2><SPAN class=editsection>[<A title="Edit section: Notes" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=26">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Notes>Notes</SPAN></H2>
<DIV class="references-small references-column-count references-column-count-2" 
style="-moz-column-count: 2; column-count: 2">
<OL class=references>
  <LI id=cite_note-0><B><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-0">^</A></B> 
  <SPAN class="citation Journal">Pearson, K. (1901). <A class="external text" 
  href="http://stat.smmu.edu.cn/history/pearson1901.pdf" rel=nofollow>"On Lines 
  and Planes of Closest Fit to Systems of Points in Space"</A> (PDF). 
  <I>Philosophical Magazine</I> <B>2</B> (6): 559–572<SPAN class=printonly>. <A 
  class="external free" href="http://stat.smmu.edu.cn/history/pearson1901.pdf" 
  rel=nofollow>http://stat.smmu.edu.cn/history/pearson1901.pdf</A></SPAN>.</SPAN><SPAN 
  class=Z3988 
  title=ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=On+Lines+and+Planes+of+Closest+Fit+to+Systems+of+Points+in+Space&amp;rft.jtitle=Philosophical+Magazine&amp;rft.aulast=Pearson%2C+K.&amp;rft.au=Pearson%2C+K.&amp;rft.date=1901&amp;rft.volume=2&amp;rft.issue=6&amp;rft.pages=559%E2%80%93572&amp;rft_id=http%3A%2F%2Fstat.smmu.edu.cn%2Fhistory%2Fpearson1901.pdf&amp;rfr_id=info:sid/en.wikipedia.org:Principal_component_analysis><SPAN 
  style="DISPLAY: none">&nbsp;</SPAN></SPAN> 
  <LI id=cite_note-1><B><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-1">^</A></B> 
  Jolliffe I.T. <A class="external text" 
  href="http://www.springer.com/west/home/new+%26+forthcoming+titles+%28default%29?SGWID=4-40356-22-2285433-0" 
  rel=nofollow>Principal Component Analysis</A>, Series: <A 
  class="external text" 
  href="http://www.springer.com/west/home/statistics/statistical+theory+and+methods?SGWID=4-10129-69-173621571-0" 
  rel=nofollow>Springer Series in Statistics</A>, 2nd ed., Springer, NY, 2002, 
  XXIX, 487 p. 28 illus. <A class="internal mw-magiclink-isbn" 
  href="http://en.wikipedia.org/wiki/Special:BookSources/9780387954424">ISBN 
  978-0-387-95442-4</A> 
  <LI id=cite_note-2><B><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-2">^</A></B> 
  A. A. Miranda, Y. A. Le Borgne, and G. Bontempi. <A class="external text" 
  href="http://www.ulb.ac.be/di/map/yleborgn/pub/NPL_PCA_07.pdf" 
  rel=nofollow>New Routes from Minimal Approximation Error to Principal 
  Components</A>, Volume 27, Number 3 / June, 2008, Neural Processing Letters, 
  Springer 
  <LI id=cite_note-3><B><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-3">^</A></B> 
  <SPAN class="citation book">Fukunaga, Keinosuke (1990). <A 
  class="external text" href="http://books.google.com/books?visbn=0122698517" 
  rel=nofollow><I>Introduction to Statistical Pattern Recognition</I></A>. 
  Elsevier<SPAN class=printonly>. <A class="external free" 
  href="http://books.google.com/books?visbn=0122698517" 
  rel=nofollow>http://books.google.com/books?visbn=0122698517</A></SPAN>.</SPAN><SPAN 
  class=Z3988 
  title=ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Introduction+to+Statistical+Pattern+Recognition&amp;rft.aulast=Fukunaga%2C+Keinosuke&amp;rft.au=Fukunaga%2C+Keinosuke&amp;rft.date=1990&amp;rft.pub=Elsevier&amp;rft_id=http%3A%2F%2Fbooks.google.com%2Fbooks%3Fvisbn%3D0122698517&amp;rfr_id=info:sid/en.wikipedia.org:Principal_component_analysis><SPAN 
  style="DISPLAY: none">&nbsp;</SPAN></SPAN> 
  <LI id=cite_note-4><B><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-4">^</A></B> 
  Jonathon Shlens, <A class="external text" 
  href="http://www.snl.salk.edu/~shlens/pca.pdf" rel=nofollow>A Tutorial on 
  Principal Component Analysis.</A> 
  <LI id=cite_note-5><B><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-5">^</A></B> 
  A.A. Miranda, Y.-A. Le Borgne, and G. Bontempi. <A class="external text" 
  href="http://www.ulb.ac.be/di/map/yleborgn/pub/NPL_PCA_07.pdf" 
  rel=nofollow>New Routes from Minimal Approximation Error to Principal 
  Components</A>, Volume 27, Number 3 / June, 2008, Neural Processing Letters, 
  Springer 
  <LI id=cite_note-6><B><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-6">^</A></B> 
  <A class="external text" 
  href="http://www.mathworks.com/access/helpdesk/help/techdoc/ref/eig.html#998306" 
  rel=nofollow>eig function</A> Matlab documentation 
  <LI id=cite_note-7><B><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-7">^</A></B> 
  <A class="external text" 
  href="http://www.mathworks.com/matlabcentral/fileexchange/24634" 
  rel=nofollow>MATLAB PCA-based Face recognition software</A> 
  <LI id=cite_note-8><B><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-8">^</A></B> 
  <A class="external text" 
  href="http://reference.wolfram.com/mathematica/ref/Eigenvalues.html" 
  rel=nofollow>Eigenvalues function</A> Mathematica documentation 
  <LI id=cite_note-roweis-9>^ <A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-roweis_9-0"><SUP><I><B>a</B></I></SUP></A> 
  <A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-roweis_9-1"><SUP><I><B>b</B></I></SUP></A> 
  Roweis, Sam. "EM Algorithms for PCA and SPCA." Advances in Neural Information 
  Processing Systems. Ed. Michael I. Jordan, Michael J. Kearns, and Sara A. 
  Solla The MIT Press, 1998. 
  <LI id=cite_note-10><B><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-10">^</A></B> 
  <SPAN class="citation Journal">Geladi, Paul; Kowalski, Bruce (1986). "Partial 
  Least Squares Regression:A Tutorial". <I>Analytica Chimica Acta</I> 
  <B>185</B>: 1–17. <A title="Digital object identifier" 
  href="http://en.wikipedia.org/wiki/Digital_object_identifier">doi</A>:<A 
  class="external text" 
  href="http://dx.doi.org/10.1016%2F0003-2670%2886%2980028-9" 
  rel=nofollow>10.1016/0003-2670(86)80028-9</A>.</SPAN><SPAN class=Z3988 
  title=ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Partial+Least+Squares+Regression%3AA+Tutorial&amp;rft.jtitle=Analytica+Chimica+Acta&amp;rft.aulast=Geladi&amp;rft.aufirst=Paul&amp;rft.au=Geladi%2C%26%2332%3BPaul&amp;rft.au=Kowalski%2C%26%2332%3BBruce&amp;rft.date=1986&amp;rft.volume=185&amp;rft.pages=1%26ndash%3B17&amp;rft_id=info:doi/10.1016%2F0003-2670%2886%2980028-9&amp;rfr_id=info:sid/en.wikipedia.org:Principal_component_analysis><SPAN 
  style="DISPLAY: none">&nbsp;</SPAN></SPAN> 
  <LI id=cite_note-11><B><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-11">^</A></B> 
  H. Zha, C. Ding, M. Gu, X. He and H.D. Simon. "Spectral Relaxation for K-means 
  Clustering", <A class="external free" 
  href="http://ranger.uta.edu/~chqding/papers/Zha-Kmeans.pdf" 
  rel=nofollow>http://ranger.uta.edu/~chqding/papers/Zha-Kmeans.pdf</A>, Neural 
  Information Processing Systems vol.14 (NIPS 2001). pp. 1057–1064, Vancouver, 
  Canada. Dec. 2001. 
  <LI id=cite_note-12><B><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-12">^</A></B> 
  C. Ding and X. He. "K-means Clustering via Principal Component Analysis". 
  Proc. of Int'l Conf. Machine Learning (ICML 2004), pp 225–232. July 2004. <A 
  class="external free" 
  href="http://ranger.uta.edu/~chqding/papers/KmeansPCA1.pdf" 
  rel=nofollow>http://ranger.uta.edu/~chqding/papers/KmeansPCA1.pdf</A> 
  <LI id=cite_note-13><B><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-13">^</A></B> 
  <SPAN class="citation book">Benzécri, J.-P. (1973). <I>L'Analyse des Données. 
  Volume II. L'Analyse des Correspondances</I>. Paris, France: 
  Dunod.</SPAN><SPAN class=Z3988 
  title=ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=L%27Analyse+des+Donn%C3%A9es.+Volume+II.+L%27Analyse+des+Correspondances&amp;rft.aulast=Benz%C3%A9cri%2C+J.-P.&amp;rft.au=Benz%C3%A9cri%2C+J.-P.&amp;rft.date=1973&amp;rft.place=Paris%2C+France&amp;rft.pub=Dunod&amp;rfr_id=info:sid/en.wikipedia.org:Principal_component_analysis><SPAN 
  style="DISPLAY: none">&nbsp;</SPAN></SPAN> 
  <LI id=cite_note-14><B><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-14">^</A></B> 
  <SPAN class="citation book">Greenacre, Michael (1983). <I>Theory and 
  Applications of Correspondence Analysis</I>. London: Academic Press. <A 
  title="International Standard Book Number" 
  href="http://en.wikipedia.org/wiki/International_Standard_Book_Number">ISBN</A>&nbsp;<A 
  title=Special:BookSources/0-12-299050-1 
  href="http://en.wikipedia.org/wiki/Special:BookSources/0-12-299050-1">0-12-299050-1</A>.</SPAN><SPAN 
  class=Z3988 
  title=ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Theory+and+Applications+of+Correspondence+Analysis&amp;rft.aulast=Greenacre%2C+Michael&amp;rft.au=Greenacre%2C+Michael&amp;rft.date=1983&amp;rft.place=London&amp;rft.pub=Academic+Press&amp;rft.isbn=0-12-299050-1&amp;rfr_id=info:sid/en.wikipedia.org:Principal_component_analysis><SPAN 
  style="DISPLAY: none">&nbsp;</SPAN></SPAN> 
  <LI id=cite_note-15><B><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-15">^</A></B> 
  <SPAN class="citation book">Le Roux, Brigitte and Henry Rouanet (2004). 
  <I>Geometric Data Analysis, From Correspondence Analysis to Structured Data 
  Analysis</I>. Dordrecht: Kluwer.</SPAN><SPAN class=Z3988 
  title=ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Geometric+Data+Analysis%2C+From+Correspondence+Analysis+to+Structured+Data+Analysis&amp;rft.aulast=Le+Roux%2C+Brigitte+and+Henry+Rouanet&amp;rft.au=Le+Roux%2C+Brigitte+and+Henry+Rouanet&amp;rft.date=2004&amp;rft.place=Dordrecht&amp;rft.pub=Kluwer&amp;rfr_id=info:sid/en.wikipedia.org:Principal_component_analysis><SPAN 
  style="DISPLAY: none">&nbsp;</SPAN></SPAN> 
  <LI id=cite_note-16><B><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-16">^</A></B> 
  A. Gorban, B. Kegl, D. Wunsch, A. Zinovyev (Eds.), <A class="external text" 
  href="http://pca.narod.ru/contentsgkwz.htm" rel=nofollow>Principal Manifolds 
  for Data Visualisation and Dimension Reduction,</A> LNCSE 58, Springer, Berlin 
  – Heidelberg – New York, 2007. <A class="internal mw-magiclink-isbn" 
  href="http://en.wikipedia.org/wiki/Special:BookSources/9783540737490">ISBN 
  978-3-540-73749-0</A> 
  <LI id=cite_note-17><B><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis#cite_ref-17">^</A></B> 
  <SPAN class="citation Journal">Kriegel, H. P.; Kröger, P.; Schubert, E.; 
  Zimek, A. (2008). <I>A General Framework for Increasing the Robustness of 
  PCA-Based Correlation Clustering Algorithms</I>. <B>5069</B>. pp. 418. <A 
  title="Digital object identifier" 
  href="http://en.wikipedia.org/wiki/Digital_object_identifier">doi</A>:<A 
  class="external text" href="http://dx.doi.org/10.1007%2F978-3-540-69497-7_27" 
  rel=nofollow>10.1007/978-3-540-69497-7_27</A>.</SPAN><SPAN class=Z3988 
  title=ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=A+General+Framework+for+Increasing+the+Robustness+of+PCA-Based+Correlation+Clustering+Algorithms&amp;rft.aulast=Kriegel&amp;rft.aufirst=H.+P.&amp;rft.au=Kriegel%2C%26%2332%3BH.+P.&amp;rft.au=Kr%C3%B6ger%2C%26%2332%3BP.&amp;rft.au=Schubert%2C%26%2332%3BE.&amp;rft.au=Zimek%2C%26%2332%3BA.&amp;rft.date=2008&amp;rft.volume=5069&amp;rft.pages=pp.+418&amp;rft_id=info:doi/10.1007%2F978-3-540-69497-7_27&amp;rfr_id=info:sid/en.wikipedia.org:Principal_component_analysis><SPAN 
  style="DISPLAY: none">&nbsp;</SPAN></SPAN> <SPAN class="plainlinks noprint" 
  style="FONT-SIZE: smaller"><A class="external text" 
  href="http://en.wikipedia.org/w/index.php?title=Template:Cite_doi/10.1007.2F978-3-540-69497-7_27&amp;action=edit&amp;editintro=Template:Cite_doi/editintro2" 
  rel=nofollow>edit</A></SPAN> </LI></OL></DIV>
<H2><SPAN class=editsection>[<A title="Edit section: See also" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=27">edit</A>]</SPAN> 
<SPAN class=mw-headline id=See_also>See also</SPAN></H2>
<DIV style="-moz-column-count: 2; column-count: 2">
<UL>
  <LI><A title="Sparse PCA" 
  href="http://en.wikipedia.org/wiki/Sparse_PCA">Sparse PCA</A> 
  <LI><A title=Biplot href="http://en.wikipedia.org/wiki/Biplot">Biplot</A> 
  <LI><A title=Eigenface 
  href="http://en.wikipedia.org/wiki/Eigenface">Eigenface</A> 
  <LI><A class=extiw title="v:Exploratory factor analysis" 
  href="http://en.wikiversity.org/wiki/Exploratory_factor_analysis">Exploratory 
  factor analysis</A> (Wikiversity) 
  <LI><A title="Factor analysis" 
  href="http://en.wikipedia.org/wiki/Factor_analysis">Factor analysis</A> 
  <LI><A title="Geometric data analysis" 
  href="http://en.wikipedia.org/wiki/Geometric_data_analysis">Geometric data 
  analysis</A> 
  <LI><A title="Factorial code" 
  href="http://en.wikipedia.org/wiki/Factorial_code">Factorial code</A> 
  <LI><A title="Independent component analysis" 
  href="http://en.wikipedia.org/wiki/Independent_component_analysis">Independent 
  component analysis</A> 
  <LI><A class=new 
  title="Time Adaptive Self-Organizing Map (page does not exist)" 
  href="http://en.wikipedia.org/w/index.php?title=Time_Adaptive_Self-Organizing_Map&amp;action=edit&amp;redlink=1">Time 
  Adaptive Self-Organizing Map</A> 
  <LI><A class=mw-redirect title="Kernel PCA" 
  href="http://en.wikipedia.org/wiki/Kernel_PCA">Kernel PCA</A> 
  <LI><A title="Matrix decomposition" 
  href="http://en.wikipedia.org/wiki/Matrix_decomposition">Matrix 
  decomposition</A> 
  <LI><A title="Nonlinear dimensionality reduction" 
  href="http://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction">Nonlinear 
  dimensionality reduction</A> 
  <LI><A title="Oja's rule" 
  href="http://en.wikipedia.org/wiki/Oja%27s_rule">Oja's rule</A> 
  <LI><A class=new title="PCA network (page does not exist)" 
  href="http://en.wikipedia.org/w/index.php?title=PCA_network&amp;action=edit&amp;redlink=1">PCA 
  network</A> 
  <LI><A class=new title="PCA applied to yield curves (page does not exist)" 
  href="http://en.wikipedia.org/w/index.php?title=PCA_applied_to_yield_curves&amp;action=edit&amp;redlink=1">PCA 
  applied to yield curves</A> 
  <LI><A title="Point distribution model" 
  href="http://en.wikipedia.org/wiki/Point_distribution_model">Point 
  distribution model</A> (PCA applied to morphometry and computer vision) 
  <LI><A title="Principal component regression" 
  href="http://en.wikipedia.org/wiki/Principal_component_regression">Principal 
  component regression</A> 
  <LI><A class=extiw 
  title="wikibooks:Statistics/Multivariate Data Analysis/Principal Component Analysis" 
  href="http://en.wikibooks.org/wiki/Statistics/Multivariate_Data_Analysis/Principal_Component_Analysis">Principal 
  component analysis</A> (Wikibooks) 
  <LI><A title="Singular spectrum analysis" 
  href="http://en.wikipedia.org/wiki/Singular_spectrum_analysis">Singular 
  spectrum analysis</A> 
  <LI><A title="Singular value decomposition" 
  href="http://en.wikipedia.org/wiki/Singular_value_decomposition">Singular 
  value decomposition</A> 
  <LI><A title="Transform coding" 
  href="http://en.wikipedia.org/wiki/Transform_coding">Transform coding</A> 
  <LI><A class=mw-redirect title="Weighted least squares" 
  href="http://en.wikipedia.org/wiki/Weighted_least_squares">Weighted least 
  squares</A> 
  <LI><A title="Dynamic mode decomposition" 
  href="http://en.wikipedia.org/wiki/Dynamic_mode_decomposition">Dynamic mode 
  decomposition</A> </LI></UL></DIV>
<H2><SPAN class=editsection>[<A title="Edit section: References" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=28">edit</A>]</SPAN> 
<SPAN class=mw-headline id=References>References</SPAN></H2>
<UL>
  <LI>R. Kramer, Chemometric Techniques for Quantitative Analysis, (1998) 
  Marcel–Dekker, <A class="internal mw-magiclink-isbn" 
  href="http://en.wikipedia.org/wiki/Special:BookSources/0824701984">ISBN 
  0-8247-0198-4</A>. 
  <LI>Shaw PJA, Multivariate statistics for the Environmental Sciences, (2003) 
  Hodder-Arnold. 
  <LI>Patra sk et al., J Photochemistry &amp; Photobiology A:Chemistry, (1999) 
  122:23–31 
  <LI><SPAN class="citation book">Jolliffe, I. T. (1986). <A 
  class="external text" 
  href="http://www.springer.com/west/home/new+%26+forthcoming+titles+%28default%29?SGWID=4-40356-22-2285433-0" 
  rel=nofollow><I>Principal Component Analysis</I></A>. Springer-Verlag. 
  pp.&nbsp;487. <A title="Digital object identifier" 
  href="http://en.wikipedia.org/wiki/Digital_object_identifier">doi</A>:<A 
  class="external text" href="http://dx.doi.org/10.1007%2Fb98835" 
  rel=nofollow>10.1007/b98835</A>. <A title="International Standard Book Number" 
  href="http://en.wikipedia.org/wiki/International_Standard_Book_Number">ISBN</A>&nbsp;<A 
  title=Special:BookSources/978-0-387-95442-4 
  href="http://en.wikipedia.org/wiki/Special:BookSources/978-0-387-95442-4">978-0-387-95442-4</A><SPAN 
  class=printonly>. <A class="external free" 
  href="http://www.springer.com/west/home/new+%26+forthcoming+titles+%28default%29?SGWID=4-40356-22-2285433-0" 
  rel=nofollow>http://www.springer.com/west/home/new+%26+forthcoming+titles+%28default%29?SGWID=4-40356-22-2285433-0</A></SPAN>.</SPAN><SPAN 
  class=Z3988 
  title=ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Principal+Component+Analysis&amp;rft.aulast=Jolliffe&amp;rft.aufirst=I.+T.&amp;rft.au=Jolliffe%2C%26%2332%3BI.+T.&amp;rft.date=1986&amp;rft.pages=pp.%26nbsp%3B487&amp;rft.pub=Springer-Verlag&amp;rft_id=info:doi/10.1007%2Fb98835&amp;rft.isbn=978-0-387-95442-4&amp;rft_id=http%3A%2F%2Fwww.springer.com%2Fwest%2Fhome%2Fnew%2B%2526%2Bforthcoming%2Btitles%2B%2528default%2529%3FSGWID%3D4-40356-22-2285433-0&amp;rfr_id=info:sid/en.wikipedia.org:Principal_component_analysis><SPAN 
  style="DISPLAY: none">&nbsp;</SPAN></SPAN> </LI></UL>
<H2><SPAN class=editsection>[<A title="Edit section: External links" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=29">edit</A>]</SPAN> 
<SPAN class=mw-headline id=External_links>External links</SPAN></H2>
<TABLE class="metadata plainlinks ambox ambox-style">
  <TBODY>
  <TR>
    <TD class=mbox-image>
      <DIV style="WIDTH: 52px"><IMG height=40 alt="" 
      src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/40px-Edit-clear.svg.png" 
      width=40></DIV></TD>
    <TD class=mbox-text>This article's use of <A 
      title="Wikipedia:External links" 
      href="http://en.wikipedia.org/wiki/Wikipedia:External_links">external 
      links</A> <B>may not follow Wikipedia's <A 
      title="Wikipedia:What Wikipedia is not" 
      href="http://en.wikipedia.org/wiki/Wikipedia:What_Wikipedia_is_not#Wikipedia_is_not_a_mirror_or_a_repository_of_links.2C_images.2C_or_media_files">policies</A> 
      or <A title="Wikipedia:External links" 
      href="http://en.wikipedia.org/wiki/Wikipedia:External_links">guidelines</A></B>. 
      Please <A class="external text" 
      href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit" 
      rel=nofollow>improve this article</A> by removing excessive and 
      inappropriate external links or by converting links into <A 
      title="Wikipedia:Citing sources" 
      href="http://en.wikipedia.org/wiki/Wikipedia:Citing_sources">footnote 
      references</A>. <SMALL><I>(May 2009)</I></SMALL></TD></TR></TBODY></TABLE>
<UL>
  <LI><A class="external text" 
  href="http://ideas.repec.org/p/pra/mprapa/12723.html" rel=nofollow>The Most 
  Representative Composite Rank Ordering of Multi-Attribute Objects by the 
  Particle Swarm Optimization</A> 
  <LI><A class="external text" href="http://ssrn.com/abstract=1321369" 
  rel=nofollow>Sub-Optimality of Rank Ordering of Objects on the Basis of the 
  Leading Principal Component Factor Scores</A> 
  <LI><A class="external text" 
  href="http://neon.otago.ac.nz/chemlect/chem306/pca/index.html" 
  rel=nofollow>Spectroscopy and PCA</A> 
  <LI><A class="external text" 
  href="http://www.utdallas.edu/~herve/abdi-wireCS-PCA2010-inpress.pdf" 
  rel=nofollow>An introduction and review of recent developments of PCA</A> 
  <LI><A class="external text" 
  href="http://www.statsoft.com/textbook/stfacan.html" rel=nofollow>An 
  introductory explanation of PCA from StatSoft</A> 
  <LI><A class="external text" href="http://www.snl.salk.edu/~shlens/pca.pdf" 
  rel=nofollow>A Tutorial on Principal Component Analysis by Jonathon Shlens</A> 
  (PDF) 
  <LI><A class="external text" 
  href="http://blog.peltarion.com/2006/06/20/the-talented-drhebb-part-2-pca/" 
  rel=nofollow>Principal Component Analysis using Hebbian learning tutorial</A> 
  <LI><A class="external text" 
  href="http://brandon-merkl.blogspot.com/2006/04/principal-components-analysis.html" 
  rel=nofollow>Presentation of Principal Component Analysis used in Biomedical 
  Engineering</A> 
  <LI><A class="external text" 
  href="http://public.lanl.gov/mewall/kluwer2002.html" rel=nofollow>Application 
  to microarray and other biomedical data</A> 
  <LI><A class="external text" href="http://feinsteinneuroscience.org/" 
  rel=nofollow>PCA in functional neuroimaging, free software</A> 
  <LI><A class="external text" 
  href="http://www.chemometry.com/Research/PCA.html" rel=nofollow>Uncertainty 
  estimation for PCA</A> 
  <LI><A class="external text" href="http://factominer.free.fr/" 
  rel=nofollow>FactoMineR, an R package dedicated to exploratory multivariate 
  analysis</A> 
  <LI><A class="external text" href="http://www.datascope.be/" rel=nofollow>A 
  web-site with presentations and open source software on exploratory 
  multivariate data analysis</A> 
  <LI><A class="external text" 
  href="http://transp-or2.epfl.ch/pagesPerso/javierFiles/software.php" 
  rel=nofollow>EasyPCA, a very simple and small PCA program under the GPL 
  license</A> 
  <LI><A class="external text" href="http://www.iiap.res.in/astrostat/" 
  rel=nofollow>R tutorial on cluster and principal component analysis including 
  example data</A> 
  <LI><A class="external text" 
  href="http://codingplayground.blogspot.com/2010/01/pca-dimensional-reduction-in-eigen.html" 
  rel=nofollow>Simple COV-based PCA using Eigen Template Library in C++</A> by 
  Antonio Gulli 
  <LI><A class="external text" href="http://www.powercam.cc/chli" 
  rel=nofollow>www.powercam.cc/chli</A> by Cheng-Hsuan Li <I>(A Chinese Tutorial 
  on Kernel Method, PCA, KPCA, LDA, GDA, and SVMs)</I> </LI></UL>
<H3><SPAN class=editsection>[<A 
title="Edit section: Non-technical introductions" 
href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit&amp;section=30">edit</A>]</SPAN> 
<SPAN class=mw-headline id=Non-technical_introductions>Non-technical 
introductions</SPAN></H3>
<UL>
  <LI><A class="external text" href="http://www.youtube.com/watch?v=BfTMmoDFXyE" 
  rel=nofollow>A layman's introduction to principal component analysis</A> 
  (video) 
  <LI><A class="external text" 
  href="http://www.models.kvl.dk/demo/pca/story/default.asp" rel=nofollow>Step 
  by step tutorial from KVL</A> (<A class="external text" 
  href="http://www.youtube.com/watch?v=4pnQd6jnCWk" rel=nofollow>video</A>) 
</LI></UL>
<TABLE class=navbox cellSpacing=0>
  <TBODY>
  <TR>
    <TD 
    style="PADDING-RIGHT: 2px; PADDING-LEFT: 2px; PADDING-BOTTOM: 2px; PADDING-TOP: 2px">
      <TABLE class="nowraplinks collapsible autocollapse" 
      style="BACKGROUND: none transparent scroll repeat 0% 0%; WIDTH: 100%" 
      cellSpacing=0>
        <TBODY>
        <TR>
          <TH class=navbox-title colSpan=2>
            <DIV style="FLOAT: left; WIDTH: 6em; TEXT-ALIGN: left">
            <DIV class="noprint plainlinks navbar" 
            style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; FONT-WEIGHT: normal; FONT-SIZE: xx-small; BACKGROUND: none transparent scroll repeat 0% 0%; PADDING-BOTTOM: 0px; BORDER-TOP-STYLE: none; PADDING-TOP: 0px; BORDER-RIGHT-STYLE: none; BORDER-LEFT-STYLE: none; BORDER-BOTTOM-STYLE: none"><A 
            title="Template:Compression methods" 
            href="http://en.wikipedia.org/wiki/Template:Compression_methods"><SPAN 
            title="View this template" 
            style="BORDER-TOP-STYLE: none; BORDER-RIGHT-STYLE: none; BORDER-LEFT-STYLE: none; BORDER-BOTTOM-STYLE: none">v</SPAN></A>&nbsp;<SPAN 
            style="FONT-SIZE: 80%">•</SPAN>&nbsp;<A 
            title="Template talk:Compression methods" 
            href="http://en.wikipedia.org/wiki/Template_talk:Compression_methods"><SPAN 
            title="Discuss this template" 
            style="BORDER-TOP-STYLE: none; BORDER-RIGHT-STYLE: none; BORDER-LEFT-STYLE: none; BORDER-BOTTOM-STYLE: none">d</SPAN></A>&nbsp;<SPAN 
            style="FONT-SIZE: 80%">•</SPAN>&nbsp;<A class="external text" 
            href="http://en.wikipedia.org/w/index.php?title=Template:Compression_methods&amp;action=edit" 
            rel=nofollow><SPAN title="Edit this template" 
            style="BORDER-TOP-STYLE: none; BORDER-RIGHT-STYLE: none; BORDER-LEFT-STYLE: none; BORDER-BOTTOM-STYLE: none">e</SPAN></A></DIV></DIV><SPAN 
            class="" style="FONT-SIZE: 110%"><A title="Data compression" 
            href="http://en.wikipedia.org/wiki/Data_compression">Data 
            compression</A> methods</SPAN></TH></TR>
        <TR style="HEIGHT: 2px">
          <TD></TD></TR>
        <TR>
          <TD class=navbox-group><A title="Lossless data compression" 
            href="http://en.wikipedia.org/wiki/Lossless_data_compression">Lossless</A></TD>
          <TD class="navbox-list navbox-odd" 
          style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; BORDER-LEFT: 2px solid; WIDTH: 100%; PADDING-TOP: 0px; TEXT-ALIGN: left">
            <DIV 
            style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"></DIV>
            <TABLE class="nowraplinks navbox-subgroup" style="WIDTH: 100%" 
            cellSpacing=0>
              <TBODY>
              <TR>
                <TD class=navbox-group 
                style="PADDING-RIGHT: 0em; PADDING-LEFT: 0em; WIDTH: 11em">
                  <DIV 
                  style="PADDING-RIGHT: 0.75em; PADDING-LEFT: 0.75em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title="Information theory" 
                  href="http://en.wikipedia.org/wiki/Information_theory">Theory</A></DIV></TD>
                <TD class="navbox-list navbox-odd" 
                style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; BORDER-LEFT: 2px solid; WIDTH: auto; PADDING-TOP: 0px; TEXT-ALIGN: left">
                  <DIV 
                  style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  class=mw-redirect title="Information entropy" 
                  href="http://en.wikipedia.org/wiki/Information_entropy">Entropy</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Kolmogorov complexity" 
                  href="http://en.wikipedia.org/wiki/Kolmogorov_complexity">Complexity</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Redundancy (information theory)" 
                  href="http://en.wikipedia.org/wiki/Redundancy_(information_theory)">Redundancy</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Lossy compression" 
                  href="http://en.wikipedia.org/wiki/Lossy_compression">Lossy</A></DIV></TD></TR>
              <TR style="HEIGHT: 2px">
                <TD></TD></TR>
              <TR>
                <TD class=navbox-group 
                style="PADDING-RIGHT: 0em; PADDING-LEFT: 0em; WIDTH: 11em">
                  <DIV 
                  style="PADDING-RIGHT: 0.75em; PADDING-LEFT: 0.75em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title="Entropy encoding" 
                  href="http://en.wikipedia.org/wiki/Entropy_encoding">Entropy 
                  encoding</A></DIV></TD>
                <TD class="navbox-list navbox-even" 
                style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; BORDER-LEFT: 2px solid; WIDTH: auto; PADDING-TOP: 0px; TEXT-ALIGN: left">
                  <DIV 
                  style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title="Shannon–Fano coding" 
                  href="http://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding">Shannon–Fano</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Shannon–Fano–Elias coding" 
                  href="http://en.wikipedia.org/wiki/Shannon%E2%80%93Fano%E2%80%93Elias_coding">Shannon–Fano–Elias</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Huffman coding" 
                  href="http://en.wikipedia.org/wiki/Huffman_coding">Huffman</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Adaptive Huffman coding" 
                  href="http://en.wikipedia.org/wiki/Adaptive_Huffman_coding">Adaptive 
                  Huffman</A><SPAN style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Arithmetic coding" 
                  href="http://en.wikipedia.org/wiki/Arithmetic_coding">Arithmetic</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Range encoding" 
                  href="http://en.wikipedia.org/wiki/Range_encoding">Range</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Golomb coding" 
                  href="http://en.wikipedia.org/wiki/Golomb_coding">Golomb</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Universal code (data compression)" 
                  href="http://en.wikipedia.org/wiki/Universal_code_(data_compression)">Universal</A> 
                  (<A title="Elias gamma coding" 
                  href="http://en.wikipedia.org/wiki/Elias_gamma_coding">Gamma</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Exponential-Golomb coding" 
                  href="http://en.wikipedia.org/wiki/Exponential-Golomb_coding">Exp-Golomb</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Fibonacci coding" 
                  href="http://en.wikipedia.org/wiki/Fibonacci_coding">Fibonacci</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Levenshtein coding" 
                  href="http://en.wikipedia.org/wiki/Levenshtein_coding">Levenshtein</A>)</DIV></TD></TR>
              <TR style="HEIGHT: 2px">
                <TD></TD></TR>
              <TR>
                <TD class=navbox-group 
                style="PADDING-RIGHT: 0em; PADDING-LEFT: 0em; WIDTH: 11em">
                  <DIV 
                  style="PADDING-RIGHT: 0.75em; PADDING-LEFT: 0.75em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title="Dictionary coder" 
                  href="http://en.wikipedia.org/wiki/Dictionary_coder">Dictionary</A></DIV></TD>
                <TD class="navbox-list navbox-odd" 
                style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; BORDER-LEFT: 2px solid; WIDTH: auto; PADDING-TOP: 0px; TEXT-ALIGN: left">
                  <DIV 
                  style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title="Run-length encoding" 
                  href="http://en.wikipedia.org/wiki/Run-length_encoding">RLE</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Byte pair encoding" 
                  href="http://en.wikipedia.org/wiki/Byte_pair_encoding">Byte 
                  pair encoding</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A title=DEFLATE 
                  href="http://en.wikipedia.org/wiki/DEFLATE">DEFLATE</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A class=mw-redirect 
                  title=Lempel–Ziv 
                  href="http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv">Lempel–Ziv</A> 
                  (<A title="LZ77 and LZ78" 
                  href="http://en.wikipedia.org/wiki/LZ77_and_LZ78">LZ77/78</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title=Lempel–Ziv–Storer–Szymanski 
                  href="http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Storer%E2%80%93Szymanski">LZSS</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title=Lempel–Ziv–Welch 
                  href="http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch">LZW</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A title=LZWL 
                  href="http://en.wikipedia.org/wiki/LZWL">LZWL</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title=Lempel–Ziv–Oberhumer 
                  href="http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Oberhumer">LZO</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Lempel–Ziv–Markov chain algorithm" 
                  href="http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Markov_chain_algorithm">LZMA</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="LZX (algorithm)" 
                  href="http://en.wikipedia.org/wiki/LZX_(algorithm)">LZX</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A title=LZRW 
                  href="http://en.wikipedia.org/wiki/LZRW">LZRW</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A title=LZJB 
                  href="http://en.wikipedia.org/wiki/LZJB">LZJB</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A class=new 
                  title="Lempel–Ziv–Stac (page does not exist)" 
                  href="http://en.wikipedia.org/w/index.php?title=Lempel%E2%80%93Ziv%E2%80%93Stac&amp;action=edit&amp;redlink=1">LZS</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A class=new 
                  title="Lempel–Ziv–Tamayo (page does not exist)" 
                  href="http://en.wikipedia.org/w/index.php?title=Lempel%E2%80%93Ziv%E2%80%93Tamayo&amp;action=edit&amp;redlink=1">LZT</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Reduced Offset Lempel Ziv" 
                  href="http://en.wikipedia.org/wiki/Reduced_Offset_Lempel_Ziv">ROLZ</A>)</DIV></TD></TR>
              <TR style="HEIGHT: 2px">
                <TD></TD></TR>
              <TR>
                <TD class=navbox-group 
                style="PADDING-RIGHT: 0em; PADDING-LEFT: 0em; WIDTH: 11em">
                  <DIV 
                  style="PADDING-RIGHT: 0.75em; PADDING-LEFT: 0.75em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em">Others</DIV></TD>
                <TD class="navbox-list navbox-even" 
                style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; BORDER-LEFT: 2px solid; WIDTH: auto; PADDING-TOP: 0px; TEXT-ALIGN: left">
                  <DIV 
                  style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title="Context tree weighting" 
                  href="http://en.wikipedia.org/wiki/Context_tree_weighting">CTW</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Burrows–Wheeler transform" 
                  href="http://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform">BWT</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A class=mw-redirect 
                  title="Prediction by Partial Matching" 
                  href="http://en.wikipedia.org/wiki/Prediction_by_Partial_Matching">PPM</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Dynamic Markov compression" 
                  href="http://en.wikipedia.org/wiki/Dynamic_Markov_compression">DMC</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Delta encoding" 
                  href="http://en.wikipedia.org/wiki/Delta_encoding">Delta</A></DIV></TD></TR></TBODY></TABLE></TD></TR>
        <TR style="HEIGHT: 2px">
          <TD></TD></TR>
        <TR>
          <TD class=navbox-group><A title="Audio compression (data)" 
            href="http://en.wikipedia.org/wiki/Audio_compression_(data)">Audio</A></TD>
          <TD class="navbox-list navbox-even" 
          style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; BORDER-LEFT: 2px solid; WIDTH: 100%; PADDING-TOP: 0px; TEXT-ALIGN: left">
            <DIV 
            style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"></DIV>
            <TABLE class="nowraplinks navbox-subgroup" style="WIDTH: 100%" 
            cellSpacing=0>
              <TBODY>
              <TR>
                <TD class=navbox-group 
                style="PADDING-RIGHT: 0em; PADDING-LEFT: 0em; WIDTH: 11em">
                  <DIV 
                  style="PADDING-RIGHT: 0.75em; PADDING-LEFT: 0.75em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title=Acoustics 
                  href="http://en.wikipedia.org/wiki/Acoustics">Theory</A></DIV></TD>
                <TD class="navbox-list navbox-odd" 
                style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; BORDER-LEFT: 2px solid; WIDTH: auto; PADDING-TOP: 0px; TEXT-ALIGN: left">
                  <DIV 
                  style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title=Companding 
                  href="http://en.wikipedia.org/wiki/Companding">Companding</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A title=Convolution 
                  href="http://en.wikipedia.org/wiki/Convolution">Convolution</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Dynamic range" 
                  href="http://en.wikipedia.org/wiki/Dynamic_range">Dynamic 
                  range</A><SPAN style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Latency (audio)" 
                  href="http://en.wikipedia.org/wiki/Latency_(audio)">Latency</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Sampling (signal processing)" 
                  href="http://en.wikipedia.org/wiki/Sampling_(signal_processing)">Sampling</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Nyquist–Shannon sampling theorem" 
                  href="http://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">Nyquist–Shannon 
                  theorem</A><SPAN style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Sound quality" 
                  href="http://en.wikipedia.org/wiki/Sound_quality">Sound 
                  quality</A></DIV></TD></TR>
              <TR style="HEIGHT: 2px">
                <TD></TD></TR>
              <TR>
                <TD class=navbox-group 
                style="PADDING-RIGHT: 0em; PADDING-LEFT: 0em; WIDTH: 11em">
                  <DIV 
                  style="PADDING-RIGHT: 0.75em; PADDING-LEFT: 0.75em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title="Audio codec" 
                  href="http://en.wikipedia.org/wiki/Audio_codec">Audio 
                  codec</A> parts</DIV></TD>
                <TD class="navbox-list navbox-even" 
                style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; BORDER-LEFT: 2px solid; WIDTH: auto; PADDING-TOP: 0px; TEXT-ALIGN: left">
                  <DIV 
                  style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title="Linear predictive coding" 
                  href="http://en.wikipedia.org/wiki/Linear_predictive_coding">LPC</A> 
                  (<A title="Log area ratio" 
                  href="http://en.wikipedia.org/wiki/Log_area_ratio">LAR</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Line spectral pairs" 
                  href="http://en.wikipedia.org/wiki/Line_spectral_pairs">LSP</A>)<SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Warped linear predictive coding" 
                  href="http://en.wikipedia.org/wiki/Warped_linear_predictive_coding">WLPC</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Code-excited linear prediction" 
                  href="http://en.wikipedia.org/wiki/Code-excited_linear_prediction">CELP</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Algebraic Code Excited Linear Prediction" 
                  href="http://en.wikipedia.org/wiki/Algebraic_Code_Excited_Linear_Prediction">ACELP</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="A-law algorithm" 
                  href="http://en.wikipedia.org/wiki/A-law_algorithm">A-law</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Μ-law algorithm" 
                  href="http://en.wikipedia.org/wiki/%CE%9C-law_algorithm">μ-law</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Adaptive DPCM" 
                  href="http://en.wikipedia.org/wiki/Adaptive_DPCM">ADPCM</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A title=DPCM 
                  href="http://en.wikipedia.org/wiki/DPCM">DPCM</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Modified discrete cosine transform" 
                  href="http://en.wikipedia.org/wiki/Modified_discrete_cosine_transform">MDCT</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Fourier transform" 
                  href="http://en.wikipedia.org/wiki/Fourier_transform">Fourier 
                  transform</A><SPAN style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  class=mw-redirect title="Psychoacoustic model" 
                  href="http://en.wikipedia.org/wiki/Psychoacoustic_model">Psychoacoustic 
                  model</A></DIV></TD></TR>
              <TR style="HEIGHT: 2px">
                <TD></TD></TR>
              <TR>
                <TD class=navbox-group 
                style="PADDING-RIGHT: 0em; PADDING-LEFT: 0em; WIDTH: 11em">
                  <DIV 
                  style="PADDING-RIGHT: 0.75em; PADDING-LEFT: 0.75em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em">Others</DIV></TD>
                <TD class="navbox-list navbox-odd" 
                style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; BORDER-LEFT: 2px solid; WIDTH: auto; PADDING-TOP: 0px; TEXT-ALIGN: left">
                  <DIV 
                  style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title="Bit rate" 
                  href="http://en.wikipedia.org/wiki/Bit_rate">Bit rate</A> (<A 
                  title="Constant bitrate" 
                  href="http://en.wikipedia.org/wiki/Constant_bitrate">CBR</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Average bitrate" 
                  href="http://en.wikipedia.org/wiki/Average_bitrate">ABR</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Variable bitrate" 
                  href="http://en.wikipedia.org/wiki/Variable_bitrate">VBR</A>)<SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A class=mw-redirect 
                  title="Speech encoding" 
                  href="http://en.wikipedia.org/wiki/Speech_encoding">Speech 
                  compression</A><SPAN style="FONT-WEIGHT: bold">&nbsp;·</SPAN> 
                  <A title="Sub-band coding" 
                  href="http://en.wikipedia.org/wiki/Sub-band_coding">Sub-band 
                  coding</A></DIV></TD></TR></TBODY></TABLE></TD></TR>
        <TR style="HEIGHT: 2px">
          <TD></TD></TR>
        <TR>
          <TD class=navbox-group><A title="Image compression" 
            href="http://en.wikipedia.org/wiki/Image_compression">Image</A></TD>
          <TD class="navbox-list navbox-odd" 
          style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; BORDER-LEFT: 2px solid; WIDTH: 100%; PADDING-TOP: 0px; TEXT-ALIGN: left">
            <DIV 
            style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"></DIV>
            <TABLE class="nowraplinks navbox-subgroup" style="WIDTH: 100%" 
            cellSpacing=0>
              <TBODY>
              <TR>
                <TD class=navbox-group 
                style="PADDING-RIGHT: 0em; PADDING-LEFT: 0em; WIDTH: 11em">
                  <DIV 
                  style="PADDING-RIGHT: 0.75em; PADDING-LEFT: 0.75em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em">Terms</DIV></TD>
                <TD class="navbox-list navbox-odd" 
                style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; BORDER-LEFT: 2px solid; WIDTH: auto; PADDING-TOP: 0px; TEXT-ALIGN: left">
                  <DIV 
                  style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title="Color space" 
                  href="http://en.wikipedia.org/wiki/Color_space">Color 
                  space</A><SPAN style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title=Pixel 
                  href="http://en.wikipedia.org/wiki/Pixel">Pixel</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Chroma subsampling" 
                  href="http://en.wikipedia.org/wiki/Chroma_subsampling">Chroma 
                  subsampling</A><SPAN style="FONT-WEIGHT: bold">&nbsp;·</SPAN> 
                  <A title="Compression artifact" 
                  href="http://en.wikipedia.org/wiki/Compression_artifact">Compression 
                  artifact</A><SPAN style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Image resolution" 
                  href="http://en.wikipedia.org/wiki/Image_resolution">Image 
                  resolution</A></DIV></TD></TR>
              <TR style="HEIGHT: 2px">
                <TD></TD></TR>
              <TR>
                <TD class=navbox-group 
                style="PADDING-RIGHT: 0em; PADDING-LEFT: 0em; WIDTH: 11em">
                  <DIV 
                  style="PADDING-RIGHT: 0.75em; PADDING-LEFT: 0.75em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em">Methods</DIV></TD>
                <TD class="navbox-list navbox-even" 
                style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; BORDER-LEFT: 2px solid; WIDTH: auto; PADDING-TOP: 0px; TEXT-ALIGN: left">
                  <DIV 
                  style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title="Run-length encoding" 
                  href="http://en.wikipedia.org/wiki/Run-length_encoding">RLE</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Fractal compression" 
                  href="http://en.wikipedia.org/wiki/Fractal_compression">Fractal</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A class=mw-redirect 
                  title="Wavelet compression" 
                  href="http://en.wikipedia.org/wiki/Wavelet_compression">Wavelet</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A class=mw-redirect 
                  title=EZW href="http://en.wikipedia.org/wiki/EZW">EZW</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Set partitioning in hierarchical trees" 
                  href="http://en.wikipedia.org/wiki/Set_partitioning_in_hierarchical_trees">SPIHT</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Pyramid (image processing)" 
                  href="http://en.wikipedia.org/wiki/Pyramid_(image_processing)">LP</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Discrete cosine transform" 
                  href="http://en.wikipedia.org/wiki/Discrete_cosine_transform">DCT</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A title="Chain code" 
                  href="http://en.wikipedia.org/wiki/Chain_code">Chain 
                  code</A><SPAN style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  class=mw-redirect title="Karhunen-Loève transform" 
                  href="http://en.wikipedia.org/wiki/Karhunen-Lo%C3%A8ve_transform">KLT</A></DIV></TD></TR>
              <TR style="HEIGHT: 2px">
                <TD></TD></TR>
              <TR>
                <TD class=navbox-group 
                style="PADDING-RIGHT: 0em; PADDING-LEFT: 0em; WIDTH: 11em">
                  <DIV 
                  style="PADDING-RIGHT: 0.75em; PADDING-LEFT: 0.75em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em">Others</DIV></TD>
                <TD class="navbox-list navbox-odd" 
                style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; BORDER-LEFT: 2px solid; WIDTH: auto; PADDING-TOP: 0px; TEXT-ALIGN: left">
                  <DIV 
                  style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title="Standard test image" 
                  href="http://en.wikipedia.org/wiki/Standard_test_image">Test 
                  images</A><SPAN style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Peak signal-to-noise ratio" 
                  href="http://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio">PSNR 
                  quality measure</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Quantization (image processing)" 
                  href="http://en.wikipedia.org/wiki/Quantization_(image_processing)">Quantization</A></DIV></TD></TR></TBODY></TABLE></TD></TR>
        <TR style="HEIGHT: 2px">
          <TD></TD></TR>
        <TR>
          <TD class=navbox-group><A title="Video compression" 
            href="http://en.wikipedia.org/wiki/Video_compression">Video</A></TD>
          <TD class="navbox-list navbox-even" 
          style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; BORDER-LEFT: 2px solid; WIDTH: 100%; PADDING-TOP: 0px; TEXT-ALIGN: left">
            <DIV 
            style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"></DIV>
            <TABLE class="nowraplinks navbox-subgroup" style="WIDTH: 100%" 
            cellSpacing=0>
              <TBODY>
              <TR>
                <TD class=navbox-group 
                style="PADDING-RIGHT: 0em; PADDING-LEFT: 0em; WIDTH: 11em">
                  <DIV 
                  style="PADDING-RIGHT: 0.75em; PADDING-LEFT: 0.75em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em">Terms</DIV></TD>
                <TD class="navbox-list navbox-odd" 
                style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; BORDER-LEFT: 2px solid; WIDTH: auto; PADDING-TOP: 0px; TEXT-ALIGN: left">
                  <DIV 
                  style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title=Video 
                  href="http://en.wikipedia.org/wiki/Video#Characteristics_of_video_streams">Video 
                  characteristics</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A title="Film frame" 
                  href="http://en.wikipedia.org/wiki/Film_frame">Frame</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A title="Frame rate" 
                  href="http://en.wikipedia.org/wiki/Frame_rate">Frame 
                  rate</A><SPAN style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title=Interlace 
                  href="http://en.wikipedia.org/wiki/Interlace">Interlace</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Video compression picture types" 
                  href="http://en.wikipedia.org/wiki/Video_compression_picture_types">Frame 
                  types</A><SPAN style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Video quality" 
                  href="http://en.wikipedia.org/wiki/Video_quality">Video 
                  quality</A><SPAN style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  class=mw-redirect title="Video resolution" 
                  href="http://en.wikipedia.org/wiki/Video_resolution">Video 
                  resolution</A></DIV></TD></TR>
              <TR style="HEIGHT: 2px">
                <TD></TD></TR>
              <TR>
                <TD class=navbox-group 
                style="PADDING-RIGHT: 0em; PADDING-LEFT: 0em; WIDTH: 11em">
                  <DIV 
                  style="PADDING-RIGHT: 0.75em; PADDING-LEFT: 0.75em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title="Video codec" 
                  href="http://en.wikipedia.org/wiki/Video_codec">Video codec 
                  parts</A></DIV></TD>
                <TD class="navbox-list navbox-even" 
                style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; BORDER-LEFT: 2px solid; WIDTH: auto; PADDING-TOP: 0px; TEXT-ALIGN: left">
                  <DIV 
                  style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title="Motion compensation" 
                  href="http://en.wikipedia.org/wiki/Motion_compensation">Motion 
                  compensation</A><SPAN style="FONT-WEIGHT: bold">&nbsp;·</SPAN> 
                  <A title="Discrete cosine transform" 
                  href="http://en.wikipedia.org/wiki/Discrete_cosine_transform">DCT</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Quantization (signal processing)" 
                  href="http://en.wikipedia.org/wiki/Quantization_(signal_processing)">Quantization</A></DIV></TD></TR>
              <TR style="HEIGHT: 2px">
                <TD></TD></TR>
              <TR>
                <TD class=navbox-group 
                style="PADDING-RIGHT: 0em; PADDING-LEFT: 0em; WIDTH: 11em">
                  <DIV 
                  style="PADDING-RIGHT: 0.75em; PADDING-LEFT: 0.75em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em">Others</DIV></TD>
                <TD class="navbox-list navbox-odd" 
                style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; BORDER-LEFT: 2px solid; WIDTH: auto; PADDING-TOP: 0px; TEXT-ALIGN: left">
                  <DIV 
                  style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
                  title="Video codec" 
                  href="http://en.wikipedia.org/wiki/Video_codec">Video 
                  codecs</A><SPAN style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  class=mw-redirect title="Rate distortion theory" 
                  href="http://en.wikipedia.org/wiki/Rate_distortion_theory">Rate 
                  distortion theory</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A title="Bit rate" 
                  href="http://en.wikipedia.org/wiki/Bit_rate">Bit rate</A> (<A 
                  title="Constant bitrate" 
                  href="http://en.wikipedia.org/wiki/Constant_bitrate">CBR</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Average bitrate" 
                  href="http://en.wikipedia.org/wiki/Average_bitrate">ABR</A><SPAN 
                  style="FONT-WEIGHT: bold">&nbsp;·</SPAN> <A 
                  title="Variable bitrate" 
                  href="http://en.wikipedia.org/wiki/Variable_bitrate">VBR</A>)</DIV></TD></TR></TBODY></TABLE></TD></TR>
        <TR style="HEIGHT: 2px">
          <TD></TD></TR>
        <TR>
          <TD class="navbox-list navbox-odd" 
          style="PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; WIDTH: 100%; PADDING-TOP: 0px" 
          colSpan=2>
            <DIV 
            style="PADDING-RIGHT: 0.25em; PADDING-LEFT: 0.25em; PADDING-BOTTOM: 0em; PADDING-TOP: 0em"><A 
            title="Timeline of information theory" 
            href="http://en.wikipedia.org/wiki/Timeline_of_information_theory">Timeline 
            of information theory, data compression, and error-correcting 
            codes</A></DIV></TD></TR>
        <TR style="HEIGHT: 2px">
          <TD></TD></TR>
        <TR>
          <TD class=navbox-abovebelow colSpan=2>See <A 
            title="Template:Compression formats" 
            href="http://en.wikipedia.org/wiki/Template:Compression_formats">Compression 
            formats</A> for formats and <A 
            title="Template:Compression software implementations" 
            href="http://en.wikipedia.org/wiki/Template:Compression_software_implementations">Compression 
            software implementations</A> for 
  codecs</TD></TR></TBODY></TABLE></TD></TR></TBODY></TABLE><!-- 
NewPP limit report
Preprocessor node count: 8963/1000000
Post-expand include size: 140373/2048000 bytes
Template argument size: 89910/2048000 bytes
Expensive parser function count: 5/500
--><!-- Saved in parser cache with key enwiki:pcache:idhash:76340-0!1!0!default!!en!4 and timestamp 20100718112131 -->
<DIV class=printfooter>Retrieved from "<A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis">http://en.wikipedia.org/wiki/Principal_component_analysis</A>"</DIV><!-- /bodytext --><!-- catlinks -->
<DIV class=catlinks id=catlinks>
<DIV id=mw-normal-catlinks><A title=Special:Categories 
href="http://en.wikipedia.org/wiki/Special:Categories">Categories</A>: <SPAN 
dir=ltr><A title="Category:Multivariate statistics" 
href="http://en.wikipedia.org/wiki/Category:Multivariate_statistics">Multivariate 
statistics</A></SPAN> | <SPAN dir=ltr><A 
title="Category:Singular value decomposition" 
href="http://en.wikipedia.org/wiki/Category:Singular_value_decomposition">Singular 
value decomposition</A></SPAN> | <SPAN dir=ltr><A title="Category:Data mining" 
href="http://en.wikipedia.org/wiki/Category:Data_mining">Data mining</A></SPAN> 
| <SPAN dir=ltr><A title="Category:Data analysis" 
href="http://en.wikipedia.org/wiki/Category:Data_analysis">Data 
analysis</A></SPAN> | <SPAN dir=ltr><A title="Category:Machine learning" 
href="http://en.wikipedia.org/wiki/Category:Machine_learning">Machine 
learning</A></SPAN></DIV>
<DIV class=mw-hidden-cats-hidden id=mw-hidden-catlinks>Hidden categories: <SPAN 
dir=ltr><A title="Category:Self-contradictory articles from May 2009" 
href="http://en.wikipedia.org/wiki/Category:Self-contradictory_articles_from_May_2009">Self-contradictory 
articles from May 2009</A></SPAN> | <SPAN dir=ltr><A 
title="Category:All self-contradictory articles" 
href="http://en.wikipedia.org/wiki/Category:All_self-contradictory_articles">All 
self-contradictory articles</A></SPAN> | <SPAN dir=ltr><A 
title="Category:All articles with unsourced statements" 
href="http://en.wikipedia.org/wiki/Category:All_articles_with_unsourced_statements">All 
articles with unsourced statements</A></SPAN> | <SPAN dir=ltr><A 
title="Category:Articles with unsourced statements from April 2010" 
href="http://en.wikipedia.org/wiki/Category:Articles_with_unsourced_statements_from_April_2010">Articles 
with unsourced statements from April 2010</A></SPAN> | <SPAN dir=ltr><A 
title="Category:Wikipedia external links cleanup" 
href="http://en.wikipedia.org/wiki/Category:Wikipedia_external_links_cleanup">Wikipedia 
external links cleanup</A></SPAN> | <SPAN dir=ltr><A 
title="Category:Wikipedia spam cleanup" 
href="http://en.wikipedia.org/wiki/Category:Wikipedia_spam_cleanup">Wikipedia 
spam cleanup</A></SPAN></DIV></DIV><!-- /catlinks -->
<DIV class=visualClear></DIV></DIV><!-- /bodyContent --></DIV><!-- /content --><!-- header -->
<DIV class=noprint id=mw-head><!-- 0 -->
<DIV class="" id=p-personal>
<H5>Personal tools</H5>
<UL>
  <LI id=pt-prefswitch-link-anon><A class=no-text-transform 
  title="Learn about new features" 
  href="http://en.wikipedia.org/w/index.php?title=Special:UsabilityInitiativePrefSwitch&amp;from=Principal_component_analysis">New 
  features</A> </LI>
  <LI id=pt-login><A 
  title="You are encouraged to log in; however, it is not mandatory. [o]" 
  accessKey=o 
  href="http://en.wikipedia.org/w/index.php?title=Special:UserLogin&amp;returnto=Principal_component_analysis">Log 
  in / create account</A> </LI></UL></DIV><!-- /0 -->
<DIV id=left-navigation><!-- 0 -->
<DIV class=vectorTabs id=p-namespaces>
<H5>Namespaces</H5>
<UL>
  <LI class=selected id=ca-nstab-main><A title="View the content page [c]" 
  accessKey=c 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis"><SPAN>Article</SPAN></A> 
  </LI>
  <LI id=ca-talk><A title="Discussion about the content page [t]" accessKey=t 
  href="http://en.wikipedia.org/wiki/Talk:Principal_component_analysis"><SPAN>Discussion</SPAN></A> 
  </LI></UL></DIV><!-- /0 --><!-- 1 -->
<DIV class="vectorMenu emptyPortlet" id=p-variants>
<H5><SPAN>Variants</SPAN><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#"></A></H5>
<DIV class=menu>
<UL></UL></DIV></DIV><!-- /1 --></DIV>
<DIV id=right-navigation><!-- 0 -->
<DIV class=vectorTabs id=p-views>
<H5>Views</H5>
<UL>
  <LI class=selected id=ca-view><A 
  href="http://en.wikipedia.org/wiki/Principal_component_analysis"><SPAN>Read</SPAN></A> 
  </LI>
  <LI id=ca-edit><A 
  title="You can edit this page. &#10;Please use the preview button before saving. [e]" 
  accessKey=e 
  href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=edit"><SPAN>Edit</SPAN></A> 
  </LI>
  <LI class="collapsible " id=ca-history><A 
  title="Past versions of this page [h]" accessKey=h 
  href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;action=history"><SPAN>View 
  history</SPAN></A> </LI></UL></DIV><!-- /0 --><!-- 1 -->
<DIV class="vectorMenu emptyPortlet" id=p-cactions>
<H5><SPAN>Actions</SPAN><A 
href="http://en.wikipedia.org/wiki/Principal_component_analysis#"></A></H5>
<DIV class=menu>
<UL></UL></DIV></DIV><!-- /1 --><!-- 2 -->
<DIV id=p-search>
<H5><LABEL for=searchInput>Search</LABEL></H5>
<FORM id=searchform action=/w/index.php><INPUT type=hidden value=Special:Search 
name=title> 
<DIV id=simpleSearch><INPUT id=searchInput title="Search Wikipedia [f]" 
accessKey=f name=search><BUTTON id=searchButton 
title="Search Wikipedia for this text" name=button type=submit>&nbsp;</BUTTON> 
</DIV></FORM></DIV><!-- /2 --></DIV></DIV><!-- /header --><!-- panel -->
<DIV class=noprint id=mw-panel><!-- logo -->
<DIV id=p-logo><A title="Visit the main page" 
style="BACKGROUND-IMAGE: url(http://upload.wikimedia.org/wikipedia/commons/d/d6/Wikipedia-logo-v2-en.png)" 
href="http://en.wikipedia.org/wiki/Main_Page"></A></DIV><!-- /logo --><!-- navigation -->
<DIV class=portal id=p-navigation>
<H5>Navigation</H5>
<DIV class=body>
<UL>
  <LI id=n-mainpage-description><A title="Visit the main page [z]" accessKey=z 
  href="http://en.wikipedia.org/wiki/Main_Page">Main page</A> </LI>
  <LI id=n-contents><A title="Guides to browsing Wikipedia" 
  href="http://en.wikipedia.org/wiki/Portal:Contents">Contents</A> </LI>
  <LI id=n-featuredcontent><A title="Featured content — the best of Wikipedia" 
  href="http://en.wikipedia.org/wiki/Portal:Featured_content">Featured 
  content</A> </LI>
  <LI id=n-currentevents><A 
  title="Find background information on current events" 
  href="http://en.wikipedia.org/wiki/Portal:Current_events">Current events</A> 
  </LI>
  <LI id=n-randompage><A title="Load a random article [x]" accessKey=x 
  href="http://en.wikipedia.org/wiki/Special:Random">Random article</A> 
</LI></UL></DIV></DIV><!-- /navigation --><!-- SEARCH --><!-- /SEARCH --><!-- interaction -->
<DIV class=portal id=p-interaction>
<H5>Interaction</H5>
<DIV class=body>
<UL>
  <LI id=n-aboutsite><A title="Find out about Wikipedia" 
  href="http://en.wikipedia.org/wiki/Wikipedia:About">About Wikipedia</A> </LI>
  <LI id=n-portal><A 
  title="About the project, what you can do, where to find things" 
  href="http://en.wikipedia.org/wiki/Wikipedia:Community_portal">Community 
  portal</A> </LI>
  <LI id=n-recentchanges><A title="The list of recent changes in the wiki [r]" 
  accessKey=r href="http://en.wikipedia.org/wiki/Special:RecentChanges">Recent 
  changes</A> </LI>
  <LI id=n-contact><A title="How to contact Wikipedia" 
  href="http://en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</A> 
  </LI>
  <LI id=n-sitesupport><A title="Support us" 
  href="http://wikimediafoundation.org/wiki/Support_Wikipedia/en">Donate to 
  Wikipedia</A> </LI>
  <LI id=n-help><A title="Guidance on how to use and edit Wikipedia" 
  href="http://en.wikipedia.org/wiki/Help:Contents">Help</A> 
</LI></UL></DIV></DIV><!-- /interaction --><!-- TOOLBOX -->
<DIV class=portal id=p-tb>
<H5>Toolbox</H5>
<DIV class=body>
<UL>
  <LI id=t-whatlinkshere><A 
  title="List of all English Wikipedia pages containing links to this page [j]" 
  accessKey=j 
  href="http://en.wikipedia.org/wiki/Special:WhatLinksHere/Principal_component_analysis">What 
  links here</A> </LI>
  <LI id=t-recentchangeslinked><A 
  title="Recent changes in pages linked from this page [k]" accessKey=k 
  href="http://en.wikipedia.org/wiki/Special:RecentChangesLinked/Principal_component_analysis">Related 
  changes</A> </LI>
  <LI id=t-upload><A title="Upload files [u]" accessKey=u 
  href="http://en.wikipedia.org/wiki/Wikipedia:Upload">Upload file</A> </LI>
  <LI id=t-specialpages><A title="List of all special pages [q]" accessKey=q 
  href="http://en.wikipedia.org/wiki/Special:SpecialPages">Special pages</A> 
  </LI>
  <LI id=t-permalink><A title="Permanent link to this revision of the page" 
  href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;oldid=371858494">Permanent 
  link</A> </LI>
  <LI id=t-cite><A title="Information on how to cite this page" 
  href="http://en.wikipedia.org/w/index.php?title=Special:Cite&amp;page=Principal_component_analysis&amp;id=371858494">Cite 
  this page</A> </LI></UL></DIV></DIV><!-- /TOOLBOX --><!-- coll-print_export -->
<DIV class=portal id=p-coll-print_export>
<H5>Print/export</H5>
<DIV class=body>
<UL id=collectionPortletList>
  <LI id=coll-create_a_book><A title="Create a book or page collection" 
  href="http://en.wikipedia.org/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Principal+component+analysis" 
  rel=nofollow>Create a book</A></LI>
  <LI id=coll-download-as-rl><A title="Download a PDF version of this wiki page" 
  href="http://en.wikipedia.org/w/index.php?title=Special:Book&amp;bookcmd=render_article&amp;arttitle=Principal+component+analysis&amp;oldid=371858494&amp;writer=rl" 
  rel=nofollow>Download as PDF</A></LI>
  <LI id=t-print><A title="Printable version of this page [p]" accessKey=p 
  href="http://en.wikipedia.org/w/index.php?title=Principal_component_analysis&amp;printable=yes">Printable 
  version</A></LI></UL></DIV></DIV><!-- /coll-print_export --><!-- LANGUAGES -->
<DIV class=portal id=p-lang>
<H5>Languages</H5>
<DIV class=body>
<UL>
  <LI class=interwiki-ar><A title="تحليل العنصر الرئيسي" 
  href="http://ar.wikipedia.org/wiki/%D8%AA%D8%AD%D9%84%D9%8A%D9%84_%D8%A7%D9%84%D8%B9%D9%86%D8%B5%D8%B1_%D8%A7%D9%84%D8%B1%D8%A6%D9%8A%D8%B3%D9%8A">العربية</A> 
  </LI>
  <LI class=interwiki-ca><A title="Anàlisi de components principals" 
  href="http://ca.wikipedia.org/wiki/An%C3%A0lisi_de_components_principals">Català</A> 
  </LI>
  <LI class=interwiki-cs><A title="Analýza hlavních komponent" 
  href="http://cs.wikipedia.org/wiki/Anal%C3%BDza_hlavn%C3%ADch_komponent">Česky</A> 
  </LI>
  <LI class=interwiki-de><A title=Hauptkomponentenanalyse 
  href="http://de.wikipedia.org/wiki/Hauptkomponentenanalyse">Deutsch</A> </LI>
  <LI class=interwiki-es><A title="Análisis de componentes principales" 
  href="http://es.wikipedia.org/wiki/An%C3%A1lisis_de_componentes_principales">Español</A> 
  </LI>
  <LI class=interwiki-eo><A title="Analizo al precipaj konsisteroj" 
  href="http://eo.wikipedia.org/wiki/Analizo_al_precipaj_konsisteroj">Esperanto</A> 
  </LI>
  <LI class=interwiki-eu><A title="Osagai nagusien analisi" 
  href="http://eu.wikipedia.org/wiki/Osagai_nagusien_analisi">Euskara</A> </LI>
  <LI class=interwiki-fa><A title="تحلیل مولفه‌های اصلی" 
  href="http://fa.wikipedia.org/wiki/%D8%AA%D8%AD%D9%84%DB%8C%D9%84_%D9%85%D9%88%D9%84%D9%81%D9%87%E2%80%8C%D9%87%D8%A7%DB%8C_%D8%A7%D8%B5%D9%84%DB%8C">فارسی</A> 
  </LI>
  <LI class=interwiki-fr><A title="Analyse en composantes principales" 
  href="http://fr.wikipedia.org/wiki/Analyse_en_composantes_principales">Français</A> 
  </LI>
  <LI class=interwiki-ko><A title="주성분 분석" 
  href="http://ko.wikipedia.org/wiki/%EC%A3%BC%EC%84%B1%EB%B6%84_%EB%B6%84%EC%84%9D">한국어</A> 
  </LI>
  <LI class=interwiki-id><A title="Analisis komponen utama" 
  href="http://id.wikipedia.org/wiki/Analisis_komponen_utama">Bahasa 
  Indonesia</A> </LI>
  <LI class=interwiki-it><A title="Analisi delle componenti principali" 
  href="http://it.wikipedia.org/wiki/Analisi_delle_componenti_principali">Italiano</A> 
  </LI>
  <LI class=interwiki-he><A title="ניתוח גורמים ראשיים" 
  href="http://he.wikipedia.org/wiki/%D7%A0%D7%99%D7%AA%D7%95%D7%97_%D7%92%D7%95%D7%A8%D7%9E%D7%99%D7%9D_%D7%A8%D7%90%D7%A9%D7%99%D7%99%D7%9D">עברית</A> 
  </LI>
  <LI class=interwiki-nl><A title=Hoofdcomponenten 
  href="http://nl.wikipedia.org/wiki/Hoofdcomponenten">Nederlands</A> </LI>
  <LI class=interwiki-ja><A title=主成分分析 
  href="http://ja.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90">日本語</A> 
  </LI>
  <LI class=interwiki-pl><A title="Analiza głównych składowych" 
  href="http://pl.wikipedia.org/wiki/Analiza_g%C5%82%C3%B3wnych_sk%C5%82adowych">Polski</A> 
  </LI>
  <LI class=interwiki-ru><A title="Метод главных компонент" 
  href="http://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D1%8B%D1%85_%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82">Русский</A> 
  </LI>
  <LI class=interwiki-fi><A title=Pääkomponenttianalyysi 
  href="http://fi.wikipedia.org/wiki/P%C3%A4%C3%A4komponenttianalyysi">Suomi</A> 
  </LI>
  <LI class=interwiki-sv><A title=Principalkomponentanalys 
  href="http://sv.wikipedia.org/wiki/Principalkomponentanalys">Svenska</A> </LI>
  <LI class=interwiki-uk><A title="Метод головних компонент" 
  href="http://uk.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B3%D0%BE%D0%BB%D0%BE%D0%B2%D0%BD%D0%B8%D1%85_%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82">Українська</A> 
  </LI>
  <LI class=interwiki-zh><A title=主成分分析 
  href="http://zh.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90">中文</A> 
  </LI></UL></DIV></DIV><!-- /LANGUAGES --></DIV><!-- /panel --><!-- footer -->
<DIV id=footer>
<UL id=footer-info>
  <LI id=footer-info-lastmod>This page was last modified on 5 July 2010 at 
  14:58.<BR>
  <LI id=footer-info-copyright>Text is available under the <A 
  href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" 
  rel=license>Creative Commons Attribution-ShareAlike License</A><A 
  style="DISPLAY: none" href="http://creativecommons.org/licenses/by-sa/3.0/" 
  rel=license></A>; additional terms may apply. See <A 
  href="http://wikimediafoundation.org/wiki/Terms_of_Use">Terms of Use</A> for 
  details.<BR>Wikipedia® is a registered trademark of the <A 
  href="http://www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</A>, a 
  non-profit organization.<BR>
  <LI class=noprint><A class=internal 
  href="http://en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact us</A> 
</LI></UL>
<UL id=footer-places>
  <LI id=footer-places-privacy><A title="wikimedia:Privacy policy" 
  href="http://wikimediafoundation.org/wiki/Privacy_policy">Privacy policy</A> 
  </LI>
  <LI id=footer-places-about><A title=Wikipedia:About 
  href="http://en.wikipedia.org/wiki/Wikipedia:About">About Wikipedia</A> </LI>
  <LI id=footer-places-disclaimer><A title="Wikipedia:General disclaimer" 
  href="http://en.wikipedia.org/wiki/Wikipedia:General_disclaimer">Disclaimers</A> 
  </LI></UL>
<UL class=noprint id=footer-icons>
  <LI id=footer-icon-poweredby><A href="http://www.mediawiki.org/"><IMG 
  height=31 alt="Powered by MediaWiki" 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/poweredby_mediawiki_88x31.png" 
  width=88></A> </LI>
  <LI id=footer-icon-copyright><A href="http://wikimediafoundation.org/"><IMG 
  height=31 alt="Wikimedia Foundation" 
  src="Principal%20component%20analysis%20-%20Wikipedia,%20the%20free%20encyclopedia_files/wikimedia-button.png" 
  width=88></A> </LI></UL>
<DIV style="CLEAR: both"></DIV></DIV><!-- /footer --><!-- fixalpha -->
<SCRIPT type=text/javascript> if ( window.isMSIE55 ) fixalpha(); </SCRIPT>
<!-- /fixalpha -->
<SCRIPT type=text/javascript>if (window.runOnloadHook) runOnloadHook();</SCRIPT>
<!-- Served by srv195 in 1.798 secs. --></BODY></HTML>
